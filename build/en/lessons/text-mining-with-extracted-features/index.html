<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="description" content="" />
		<link rel="icon" href="/favicon.png" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		

		

		<link rel="stylesheet" href="/_app/assets/start-61d1577b.css">
		<link rel="modulepreload" href="/_app/start-c09d08cd.js">
		<link rel="modulepreload" href="/_app/chunks/vendor-9b9d6288.js">
		<link rel="modulepreload" href="/_app/pages/__layout.svelte-de46afb2.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/__layout.svelte-e75718c6.js">
		<link rel="modulepreload" href="/_app/chunks/stores-191d1505.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js">

		<script type="module">
			import { start } from "/_app/start-c09d08cd.js";
			start({
				target: document.querySelector("#svelte"),
				paths: {"base":"","assets":""},
				session: {},
				route: true,
				spa: false,
				trailing_slash: "never",
				hydrate: {
					status: 200,
					error: null,
					nodes: [
						import("/_app/pages/__layout.svelte-de46afb2.js"),
						import("/_app/pages/_lang_/__layout.svelte-e75718c6.js"),
						import("/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js")
					],
					url: new URL("sveltekit://prerender/en/lessons/text-mining-with-extracted-features"),
					params: {lang:"en",lessons:"lessons",slug:"text-mining-with-extracted-features"}
				}
			});
		</script><script>
			if ('serviceWorker' in navigator) {
				navigator.serviceWorker.register('/service-worker.js');
			}
		</script>
	</head>
	<body>
		<div id="svelte">


The Programming Historian <a href="/en">English</a> <a href="/es">Spanish</a>
<br>
This is the en edition.

<h1>Text Mining in Python through the HTRC Feature Reader</h1>

<!-- HTML_TAG_START --><p>{% include toc.html %}</p>
<p>Summary: <em>We introduce a toolkit for working with the 13.6 million volume Extracted Features Dataset from the HathiTrust Research Center. You will learn how to peer at the words and trends of any book in the collection, while developing broadly useful Python data analysis skills.</em></p>
<p>The <a href="https://www.hathitrust.org">HathiTrust</a> holds nearly 15 million digitized volumes from libraries around the world. In addition to their individual value, these works in aggregate are extremely valuable for historians. Spanning many centuries and genres, they offer a way to learn about large-scale trends in history and culture, as well as evidence for changes in language or even the structure of the book. To simplify access to this collection the HathiTrust Research Center (HTRC) has released the Extracted Features dataset (Capitanu et al. 2015): a dataset that provides quantitative information describing every page of every volume in the collection.</p>
<p>In this lesson, we introduce the HTRC Feature Reader, a library for working with the HTRC Extracted Features dataset using the Python programming language. The HTRC Feature Reader is structured to support work using popular data science libraries, particularly Pandas. Pandas provides simple structures for holding data and powerful ways to interact with it. The HTRC Feature Reader uses these data structures, so learning how to use it will also cover general data analysis skills in Python.</p>
<p>Today, you&#39;ll learn:</p>
<ul>
<li>How to work with <em>notebooks</em>, an interactive environment for data science in Python;</li>
<li>Methods to read and visualize text data for millions of books with the HTRC Feature Reader; and</li>
<li>Data malleability, the skills to select, slice, and summarize extracted features data using the flexible &quot;DataFrame&quot; structure.</li>
</ul>
<h2 id="background">Background</h2>
<p>The <strong>HathiTrust Research Center</strong> (<strong>HTRC</strong>) is the research arm of the HathiTrust, tasked with supporting research usage of the works held by the HathiTrust. Particularly, this support involves mediating large-scale access to materials in a non-consumptive manner, which aims to allow research over a work without enabling that work to be traditionally enjoyed or read by a human reader.  Huge digital collections can be of public benefit by allowing scholars to discover insights about history and culture, and the non-consumptive model allows for these uses to be sought within the restrictions of intellectual property law.</p>
<p>As part of its mission, the HTRC has released the <strong>Extracted Features</strong> (<strong>EF</strong>) dataset containing features derived for every page of 13.6 million &#39;volumes&#39; (a generalized term referring to the different types of materials in the HathiTrust collection, of which books are the most prevalent type).</p>
<p>What is a feature? A <strong>feature</strong> is a quantifiable marker of something measurable, a datum. A computer cannot understand the meaning of a sentence implicitly, but it can understand the counts of various words and word forms, or the presence or absence of stylistic markers, from which it can be trained to better understand text. Many text features are non-consumptive in that they don&#39;t retain enough information to reconstruct the book text.</p>
<p>Not all features are useful, and not all algorithms use the same features. With the HTRC EF Dataset, we have tried to include the most generally useful features, as well as adapt to scholarly needs. We include per-page information such as counts of words tagged by part of speech (e.g. <em>how many times does the word <code>jaguar</code> appear as a lowercase noun on this page</em>), line and sentence counts, and counts of characters at the leftmost and rightmost sides of a page. No positional information is provided, so the data would not specify if &#39;brown&#39; is followed by &#39;dog&#39;, though the information is shared for every single page, so you can at least infer how often &#39;brown&#39; and &#39;dog&#39; occurred in the same general vicinity within a text.</p>
<p>Freely accessible and preprocessed, the Extracted Features dataset offers a great entry point to programmatic text analysis and text mining. To further simplify beginner usage, the HTRC has released the HTRC Feature Reader. The <strong>HTRC Feature Reader</strong> scaffolds use of the dataset with the Python programming language.</p>
<p>This tutorial teaches the fundamentals of using the Extracted Features dataset with the HTRC Feature Reader. The HTRC Feature Reader is designed to make use of data structures from the most popular scientific tools in Python, so the skills taught here will apply to other settings of data analysis. In this way, the Extracted Features dataset is a particularly good use case for learning more general text analysis skills. We will look at data structures for holding text, patterns for querying and filtering that information, and ways to summarize, group, and visualize the data.</p>
<h2 id="possibilities">Possibilities</h2>
<p>Though it is relatively new, the Extracted Features dataset is already seeing use by scholars, as seen on a <a href="https://wiki.htrc.illinois.edu/display/COM/Extracted+Features+in+the+Wild">page collected by the HTRC</a>.</p>
<p><a href="https://doi.org/10.6084/m9.figshare.1279201">Underwood</a> leveraged the features for identifying genres, such as fiction, poetry, and drama (2014). Associated with this work, he has released a dataset of 178k books classified by genre alongside genre-specific word counts (<a href="https://doi.org/10.13012/J8JW8BSJ">Underwood 2015</a>).</p>
<p>The Underwood subset of the Extracted Features dataset was used by Forster (2015) to <a href="http://cforster.com/2015/09/gender-in-hathitrust-dataset/">observing gender in literature</a>, illustrating the decline of woman authors through the 19th century.</p>
<p>The Extracted Features dataset also underlies higher-level analytic tools. <a href="http://mimno.infosci.cornell.edu/wordsim/nearest.html">Mimno</a> processed word co-occurrence tables per year, allowing others to view how correlations between topics change over time (2014). The <a href="https://analytics.hathitrust.org/bookworm">HT Bookworm</a> project has developed an API and visualization tools to support exploration of trends within the HathiTrust collection across various classes, genres, and languages. Finally, we have developed an approach to <a href="https://github.com/organisciak/htrc-book-models">within-book topic modelling</a> which functions as a mnemonic accompaniment to a previously-read book (Organisciak 2014).</p>
<h2 id="suggested-prior-skills">Suggested Prior Skills</h2>
<p>This lesson provides a gentle but technical introduction to text analysis in Python with the HTRC Feature Reader. Most of the code is provided, but is most useful if you are comfortable tinkering with it and seeing how outputs change when you do.</p>
<p>We recommend a baseline knowledge of Python conventions, which can be learned with Turkel and Crymble&#39;s <a href="/lessons/introduction-and-installation">series of Python lessons</a> on Programming Historian.</p>
<p>The skills taught here are focused on flexibly accessing and working with already-computed text features. For a better understanding of the process of deriving word features, Programming Historian provides a lesson on <a href="/lessons/counting-frequencies">Counting Frequencies</a>, by Turkel and Crymble.</p>
<p>A more detailed look at text analysis with Python is provided in the <a href="https://github.com/sgsinclair/alta/blob/master/ipynb/ArtOfLiteraryTextAnalysis.ipynb">Art of Literary Text Analysis</a> (Sinclair). The Art of Literary Text Analysis (ALTA) provides a deeper introduction to foundation Python skills, as well as introduces further text analytics concepts to accompany the skills we cover in this lesson. This includes lessons on extracting features (<a href="https://github.com/sgsinclair/alta/blob/master/ipynb/Nltk.ipynb">tokenization</a>, <a href="https://github.com/sgsinclair/alta/blob/master/ipynb/RepeatingPhrases.ipynb">collocations</a>), and <a href="https://github.com/sgsinclair/alta/blob/master/ipynb/GettingGraphical.ipynb">visualizing trends</a>.</p>
<h1 id="download-the-lesson-files">Download the Lesson Files</h1>
<p>To follow along, download <a href="https://github.com/programminghistorian/ph-submissions/raw/gh-pages/assets/extracted-features-lesson_files.zip">lesson_files.zip</a> and unzip it to any directory you choose.</p>
<p>The lesson files include a sample of files from the HTRC Extracted Features dataset. After you learn to use the feature data in this lesson, you may want to work with the entirety of the dataset. The details on how to do this are described in <a href="#appendix-downloading-custom-files-via-rsync">Appendix: rsync</a>.</p>
<h2 id="installation">Installation</h2>
<p>For this lesson, you need to install the HTRC Feature Reader library for Python alongside the data science libraries that it depends on.</p>
<p>For ease, this lesson will focus on installing Python through a scientific distribution called Anaconda. Anaconda is an easy-to-install Python distribution that already includes most of the dependencies for the HTRC Feature Reader.</p>
<p>To install Anaconda, download the installer for your system from the <a href="https://www.continuum.io">Anaconda download page</a> and follow their instructions for installation of either the Windows 64-bit Graphical Installer or the Mac OS X 64-bit Graphical Installer. You can choose either version of Python for this lesson. If you have followed earlier lessons on Python at the <em>Programming Historian</em>, you are using Python 2, but the HTRC Feature Reader also supports Python 3.</p>
<p>{% include figure.html filename=&quot;conda-install.PNG&quot; caption=&quot;Conda Install&quot; %}</p>
<h3 id="installing-the-htrc-feature-reader">Installing the HTRC Feature Reader</h3>
<p>The HTRC Feature Reader can be installed by command line. First open a terminal application:</p>
<ul>
<li><em>Windows</em>: Open &#39;Command Prompt&#39; from the Start Menu and type: <code>activate</code>.</li>
<li><em>Mac OS/Linux</em>: Open &#39;Terminal&#39; from Applications and type <code>source activate</code>.</li>
</ul>
<p>If Anaconda was properly installed, you should see something similar to this:</p>
<p>{% include figure.html filename=&quot;activating-env.PNG&quot; caption=&quot;Activating the default Anaconda environment.&quot; %}</p>
<p>Now, you need to type one command:</p>
<pre><code class="language-bash">conda install -c htrc htrc-feature-reader
</code></pre>
<p>This command installs the HTRC Feature Reader and its necessary dependencies. We specify <code>-c htrc</code> so the installation command knows to find the library from the <code>htrc</code> organization.</p>
<p>That&#39;s it! At this point you have everything necessary to start reading HTRC Feature Reader files.</p>
<blockquote>
<p><em>psst</em>, advanced users: You can install the HTRC Feature Reader <em>without</em> Anaconda with <code>pip install htrc-feature-reader</code>, though for this lesson you&#39;ll need to install two additional libraries <code>pip install matplotlib jupyter</code>. Also, note that not all manual installations are alike because of hard-to-configure system optimizations: this is why we recommend Anaconda. If you think your code is going slow, you should check that Numpy has access to <a href="http://stackoverflow.com/a/19350234/233577">BLAS and LAPACK libraries</a> and install <a href="http://pandas.pydata.org/pandas-docs/version/0.15.2/install.html#recommended-dependencies">Pandas recommended packages</a>. The rest is up to you, advanced user!</p>
</blockquote>
<h2 id="start-a-notebook">Start a Notebook</h2>
<p>Using Python the traditional way -- writing a script to a file and running it -- can become clunky for text analysis, where the ability to look at and interact with data is invaluable.
This lesson uses an alternative approach: Jupyter notebooks.</p>
<p>Jupyter gives you an interactive version of Python (called IPython) that you can access in a &quot;notebook&quot; format in your web browser. This format has many benefits. The interactivity means that you don&#39;t need to re-run an entire script each time: you can run or re-run blocks of code as you go along, without losing your enviroment (i.e. the variables and code that are already loaded). The notebook format makes it easier to examine bits of information as you go along, and allows for text blocks to intersperse a narrative.</p>
<p>Jupyter was installed alongside Anaconda in the previous section, so it should be available to load now.</p>
<p>From the Start Menu (Windows) or Applications directory (Mac OS), open &quot;Jupyter notebook&quot;. This will start Jupyter on your computer and open a browser window. Keep the console window in the background, the browser is where the magic happens.</p>
<p>{% include figure.html filename=&quot;open-notebook.PNG&quot; caption=&quot;Opening Jupyter on Windows&quot; %}</p>
<p>If your web browser does not open automatically, Jupyter can be accessed by going to the address &quot;localhost:8888&quot; - or a different port number, which is noted in the console (&quot;The Jupyter Notebook is running at...&quot;):</p>
<p>{% include figure.html filename=&quot;notebook-start.png&quot; caption=&quot;A freshly started Jupyter notebook instance.&quot; %}</p>
<p>Jupyter is now showing a directory structure from your home folder. Navigate to the lesson folder where you unzipped <a href="https://github.com/programminghistorian/ph-submissions/raw/gh-pages/assets/extracted-features-lesson_files.zip">lesson_files.zip</a>.</p>
<p>In the lesson folder, open <code>Start Here.pynb</code>: your first notebook!</p>
<p>{% include figure.html filename=&quot;notebook-hello-world.png&quot; caption=&quot;Hello world in a notebook&quot; %}</p>
<p>Here there are instructions for editing a cell of text or code, and running it. Try editing and running a cell, and notice that it only affects itself. Here are a few tips for using the notebook as the lesson continues:</p>
<ul>
<li>New cells are created with the <i class="fa-plus fa"> Plus</i> button in the toolbar. When not editing, this can be done by pressing &#39;b&#39; on your keyboard.</li>
<li>New cells are &quot;code&quot; cells by default, but can be changed to &quot;Markdown&quot; (a type of text input) in a dropdown menu on the toolbar. In edit mode, you can paste in code from this lesson or type it yourself.</li>
<li>Switching a cell to edit mode is done by pressing Enter.</li>
<li>Running a cell is done by clicking <i class="fa-step-forward fa"> Play</i> in the toolbar, or with <code>Ctrl+Enter</code> (<code>Ctrl+Return</code> on Mac OS). To run a cell and immediately move forward, use <code>Shift+Enter</code> instead.</li>
</ul>
<blockquote>
<p>An example of a full-fledged notebook is included with the lesson files in <code>example/Lesson Draft.ipynb</code>.</p>
</blockquote>
<p>In this notebook, it&#39;s time to give the HTRC Feature Reader a try. When it is time to try some code, start a new cell with <i class="fa-plus fa"> Plus</i>, and run the code with <i class="fa-step-forward fa"> Play</i>. Before continuing, click on the title to change it to something more descriptive than &quot;Start Here&quot;.</p>
<h2 id="reading-your-first-volume">Reading your First Volume</h2>
<p>The HTRC Feature Reader library has three main objects: <strong>FeatureReader</strong>, <strong>Volume</strong>, and <strong>Page</strong>.</p>
<p>The <strong>FeatureReader</strong> object is the interface for loading the dataset files and making sense of them. The files are originally formatted in a notation called JSON (which <em>Programming Historian</em> discusses <a href="/lessons/json-and-jq">here</a>) and compressed, which FeatureReader makes sense of and returns as Volume objects. A <strong>Volume</strong> is a representation of a single book or other work. This is where you access features about a work. Many features for a volume are collected from individual pages; to access Page information, you can use the <strong>Page</strong> object.</p>
<p>Let&#39;s load two volumes to understand how the FeatureReader works. Create a cell in the already-open Jupyter notebook and run the following code. This should give you the input shown below.</p>
<pre><code class="language-python">from htrc_features import FeatureReader
import os
paths = [os.path.join(&#39;data&#39;, &#39;sample-file1.json.bz2&#39;), os.path.join(&#39;data&#39;, &#39;sample-file2.json.bz2&#39;)]
fr = FeatureReader(paths)
for vol in fr.volumes():
    print(vol.title)
</code></pre>
<pre><code>June / by Edith Barnard Delano ; with illustrations.
You never know your luck; being the story of a matrimonial deserter, by Gilbert Parker ... illustrated by W.L. Jacobs.
</code></pre>
<p>Here, the FeatureReader is imported and initialized with file paths pointing to two Extracted Features files. The files are in a directory called &#39;data&#39;. Different systems do file paths differently: Windows uses back slashes (&#39;data\...&#39;) while Linux and Mac OS use forward slashes (&#39;data/...&#39;). <code>os.path.join</code> is used to make sure that the file path is correctly structured, a convention to ensure that code works on these different platforms.</p>
<p>With <code>fr = FeatureReader(paths)</code>, the FeatureReader is initialized, meaning it is ready to use. An initialized FeatureReader is holding references to the file paths that we gave it, and will load them into Volume objects when asked.</p>
<p>Consider the last bit of code:</p>
<pre><code class="language-python">for vol in fr.volumes():
    print(vol.title)
</code></pre>
<p>This code asks for volumes in a way that can be iterated through. The <code>for</code> loop is saying to <code>fr.volumes()</code>, &quot;give me every single volume that you have, one by one.&quot; Each time the <code>for</code> loop gets a volume, it starts calling it <code>vol</code>, runs what is inside the loop on it, then asks for the next one. In this case, we just told it to print the title of the volume.</p>
<p>You may recognize <code>for</code> loops from past experience iterating through what is known as a <code>list</code> in Python. However, it is important to note that <code>fr.volumes()</code> is <em>not</em> a list. If you try to access it directly, it won&#39;t print all the volumes; rather, it identifies itself as something known as a generator:</p>
<p>{% include figure.html filename=&quot;generator.png&quot; caption=&quot;Identifying a generator&quot; %}</p>
<p>What is a generator, and why do we iterate over it?</p>
<p>Generators are the key to working with lots of data. They allow you to iterate over a set of items that don&#39;t exist yet, preparing them only when it is their turn to be acted upon.</p>
<p>Remember that there are 13.6 million volumes in the Extracted Features dataset. When coding at that scale, you need to be be mindful of two rules:</p>
<ol>
<li>Don&#39;t hold everything in memory: you can&#39;t. Use it, reduce it, and move on.</li>
<li>Don&#39;t devote cycles to processing something before you need it.</li>
</ol>
<p>A generator simplifies such on-demand, short term usage. Think of it like a pizza shop making pizzas when a customer orders, versus one that prepares them beforehand. The traditional approach to iterating through data is akin to making <em>all</em> the pizzas for the day before opening. Doing so would make the buying process quicker, but also adds a huge upfront time cost, needs larger ovens, and necessitates the space to hold all the pizzas at once. An alternate approach is to make pizzas on-demand when customers buy them, allowing the pizza place to work with smaller capacities and without having pizzas laying around the shop. This is the type of approach that a generator allows.</p>
<p>Volumes need to be prepared before you do anything with them, being read, decompressed and parsed. This &#39;initialization&#39; of a volume is done when you ask for the volume, <em>not</em> when you create the FeatureReader. In the above code, after you run <code>fr = FeatureReader(paths)</code>, there are are still no <code>Volume</code> objects held behind the scenes: just the references to the file locations. The files are only read when their time comes in the loop on the generator <code>fr.volumes()</code>. Note that because of this one-by-one reading, the items of a generator cannot be accessed out of order (e.g. you cannot ask for the third item of <code>fr.volumes()</code> without going through the first two first).</p>
<h2 id="whats-in-a-volume">What&#39;s in a Volume?</h2>
<p>Let&#39;s take a closer look at what features are accessible for a Volume object. For clarity, we&#39;ll grab the first Volume to focus on, which can conveniently be accessed with the <code>first()</code> method. Any code you write can easily be run later with a <code>for vol in fr.volumes()</code> loop.</p>
<p>Again here, start a new code cell in the same notebook that you had open before and run the following code. The FeatureReader does not need to be loaded again: it is still initialized and accessible as <code>fr</code> from earlier.</p>
<pre><code class="language-python"># Reading a single volume
vol = fr.first()
vol
</code></pre>
<pre><code>&lt;htrc_features.feature_reader.Volume at 0x1cf355a60f0&gt;
</code></pre>
<p>While the majority of the HTRC Extracted Features dataset is <em>features</em>, quantitative abstractions of a book&#39;s written content, there is also a small amount of metadata included for each volume. We already saw <code>Volume.title</code> accessed earlier. Other metadata attributes include:</p>
<ul>
<li><code>Volume.id</code>: A unique identifier for the volume in the HathiTrust and the HathiTrust Research Center.</li>
<li><code>Volume.year</code>: The publishing date of the volume.</li>
<li><code>Volume.language</code>: The classified language of the volume.</li>
<li><code>Volume.oclc</code>: The OCLC control number(s).</li>
</ul>
<p>The volume id can be used to pull more information from other sources. The scanned copy of the book can be found from the HathiTrust Digital Library, when available, by accessing <code>http://hdl.handle.net/2027/{VOLUME ID}</code>. In the feature reader, this url is retrieved by calling <code>vol.handle_url</code>:</p>
<pre><code class="language-python">print(vol.handle_url)
</code></pre>
<pre><code>http://hdl.handle.net/2027/nyp.33433075749246
</code></pre>
<p>{% include figure.html filename=&quot;June-cover.PNG&quot; caption=&quot;Digital copy of sample book&quot; %}</p>
<p>Hopefully by now you are growing more comfortable with the process of running code in a Jupyter notebook, starting a cell, writing code, and running the cell. A valuable property of this type of interactive coding is that there is room for error. An error doesn&#39;t cause the whole program to crash, requiring you to rerun everything from the start. Instead, just fix the code in your cell and try again.</p>
<p>In Jupyter, pressing the &#39;TAB&#39; key will guess at what you want to type next. Typing <code>vo</code> then TAB will fill in <code>vol</code>, typing <code>Fea</code> then TAB will fill in <code>FeatureReader</code>.</p>
<p>Auto-completion with the tab key also provides more information about what you can get from an object. Try typing <code>vol.</code> (with the period) in a new cell, then press TAB. Jupyter shows everything that you can access for that Volume.</p>
<p>{% include figure.html filename=&quot;autocomplete.png&quot; caption=&quot;Tab Autocomplete in Jupyter&quot; %}</p>
<p>The Extracted Features dataset does not hold all the metadata that the HathiTrust has for the book. More in-depth metadata like genre and subject class needs to be grabbed from other sources, such as the <a href="https://www.hathitrust.org/bib_api">HathiTrust Bibliographic API</a>. The URL to access this information can be retrieved with <code>vol.ht_bib_url</code>.</p>
<h2 id="our-first-feature-access-visualizing-words-per-page">Our First Feature Access: Visualizing Words Per Page</h2>
<p>It&#39;s time to access the first features of <code>vol</code>: a table of total words for every single page. These can be accessed by calling <code>vol.tokens_per_page()</code>. Try the following code.</p>
<blockquote>
<p>If you are using a Jupyter notebook, returning this table at the end of a cell formats it nicely in the browser. Below, you&#39;ll see us append <code>.head()</code> to the <code>tokens</code> table, which allows us to look at just the top few rows: the &#39;head&#39; of the data.</p>
</blockquote>
<pre><code class="language-python">tokens = vol.tokens_per_page()
# Show just the first few rows, so we can look at what it looks like
tokens.head()
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>count</th>
    </tr>
    <tr>
      <th>page</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>5</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<blockquote>
<p>No print! We didn&#39;t call &#39;print()&#39; to make Jupyter show the table. Instead, it automatically guessed that you want to display the information from the last code line of the cell.</p>
</blockquote>
<p>This is a straightforward table of information, similar to what you would see in Excel or Google Spreadsheets. Listed in the table are page numbers and the count of words on each page. With only two dimensions, it is trivial to plot the number of words per page. The table structure holding the data has a <code>plot</code> method for data graphics. Without extra arguments, <code>tokens.plot()</code> will assume that you want a line chart with the page on the x-axis and word count on the y-axis.</p>
<pre><code class="language-python">%matplotlib inline
tokens.plot()
</code></pre>
<p>{% include figure.html filename=&quot;draft_23_1.png&quot; caption=&quot;Output graph.&quot; %}</p>
<blockquote>
<p><code>%matplotlib inline</code> tells Jupyter to show the plotted image directly in the notebook web page. It only needs to be called once, and isn&#39;t needed if you&#39;re not using notebooks.</p>
</blockquote>
<p>On some systems, this may take some time the first time. It is clear that pages at the start of a book have fewer words per page, after which the count is fairly steady except for occasional valleys.</p>
<p>You may have some guesses for what these patterns mean. A look at the <a href="http://hdl.handle.net/2027/nyp.33433074811310">scans</a> confirms that the large valleys are often illustration pages or blank pages, small valleys are chapter headings, and the upward pattern at the start is from front matter.</p>
<p>Not all books will have the same patterns so we can&#39;t just codify these correlations for millions of books. However, looking at this plot makes clear an inportant assumption in text and data mining: that there are patterns underlying even the basic statistics derived from a text. The trick is to identify the consistent and interesting patterns and teach them to a computer.</p>
<h3 id="understanding-dataframes">Understanding DataFrames</h3>
<p>Wait... how did we get here so quickly!? We went from a volume to a data visualization in two lines of code. The magic is in the data structure used to hold our table of data: a DataFrame.</p>
<p>A <strong>DataFrame</strong> is a type of object provided by the data analysis library, Pandas. <strong>Pandas</strong> is very common for data analysis, allowing conveniences in Python that are found in statistical languages like R or Matlab.</p>
<p>In the first line, <code>vol.tokens_per_page()</code> returns a DataFrame, something that can be confirmed if you ask Python about its type with <code>type(tokens)</code>. This means that <em>after setting <code>tokens</code>, we&#39;re no longer working with HTRC-specific code, just book data held in a common and very robust table-like construct from Pandas</em>. <code>tokens.head()</code> used a DataFrame method to look at the first few rows of the dataset, and <code>tokens.plot()</code> uses a method from Pandas to visualize data.</p>
<p>Many of the methods in the HTRC Feature Reader return DataFrames. The aim is to fit into the workflow of an experienced user, rather than requiring them to learn proprietary new formats. For new Python data mining users, learning to use the HTRC Feature Reader means learning many data mining skills that will translate to other uses.</p>
<h2 id="loading-a-token-list">Loading a Token List</h2>
<p>The information contained in <code>vol.tokens_per_page()</code> is minimal, a sum of all words in the body of each page.
The Extracted Features dataset also provides token counts with much more granularity: for every part of speech (e.g. noun, verb) of every occurring capitalization of every word of every section (i.e. header, footer, body) of every page of the volume.</p>
<p><code>tokens_per_page()</code> only kept the &quot;for every page&quot; grouping; <code>vol.tokenlist()</code> can be called to return section-, part-of-speech-, and word-specific details:</p>
<pre><code class="language-python">tl = vol.tokenlist()
# Let&#39;s look at some words deeper into the book:
# from 1000th to 1100th row, skipping by 15 [1000:1100:15]
tl[1000:1100:15]
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th>count</th>
    </tr>
    <tr>
      <th>page</th>
      <th>section</th>
      <th>token</th>
      <th>pos</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="2" valign="top">27</th>
      <th rowspan="2" valign="top">body</th>
      <th>those</th>
      <th>DT</th>
      <td>1</td>
    </tr>
    <tr>
      <th>within</th>
      <th>IN</th>
      <td>1</td>
    </tr>
    <tr>
      <th rowspan="5" valign="top">28</th>
      <th rowspan="5" valign="top">body</th>
      <th>a</th>
      <th>DT</th>
      <td>3</td>
    </tr>
    <tr>
      <th>be</th>
      <th>VB</th>
      <td>1</td>
    </tr>
    <tr>
      <th>deserted</th>
      <th>VBN</th>
      <td>1</td>
    </tr>
    <tr>
      <th>faintly</th>
      <th>RB</th>
      <td>1</td>
    </tr>
    <tr>
      <th>important</th>
      <th>JJ</th>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<p>As before, the data is returned as a Pandas DataFrame. This time, there is much more information. Consider a single row:</p>
<p>{% include figure.html filename=&quot;single-row-tokencount.png&quot; caption=&quot;Single row of tokenlist.&quot; %}</p>
<p>The columns in bold are an index. Unlike the typical one-dimensional index seen before, here there are four dimensions to the index: page, section, token, and pos. This row says that for the 24th page, in the body section (i.e. ignoring any words in the header or footer), the word &#39;years&#39; occurs 1 time as an plural noun. The part-of-speech tag for a plural noun, <code>NNS</code>, follows the <a href="https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html">Penn Treebank</a> definition.</p>
<blockquote>
<p>The &quot;words&quot; on the first page seems to be OCR errors for the cover of the book. The HTRC Feature Reader refers to &quot;pages&quot; as the $$n^{th}$$ scanned image of the volume, not the actual number printed on the page. This is why &quot;page 1&quot; for this example is the cover.</p>
</blockquote>
<p>Tokenlists can be retrieved with arguments that combine information by certain dimensions, such as <code>case</code>, <code>pos</code>, or <code>page</code>. For example, <code>case=False</code> specified that &quot;Jaguar&quot; and &quot;jaguar&quot; should be counted together. You may also notice that, by default, only &#39;body&#39; is returned, a default that can be overridden.</p>
<p>Look at the following list of commands: can you guess what the output will look like? Try for yourself and observe how the output changes.</p>
<ul>
<li><code>vol.tokenlist(case=False)</code></li>
<li><code>vol.tokenlist(pos=False)</code></li>
<li><code>vol.tokenlist(pages=False, case=False, pos=False)</code></li>
<li><code>vol.tokenlist(section=&#39;header&#39;)</code></li>
<li><code>vol.tokenlist(section=&#39;group&#39;)</code></li>
</ul>
<p>Details for these arguments are available in the code <a href="http://htrc.github.io/htrc-feature-reader/htrc_features/feature_reader.m.html#htrc_features.feature_reader.Volume.tokenlist">documentation</a> for the Feature Reader.</p>
<p>Jupyter provides another convenience here. Documentation can be accessed within the notebook by adding a &#39;?&#39; to the start of a piece of code. Try it with <code>?vol.tokenlist</code>, or with other objects or variables.</p>
<h2 id="working-with-dataframes">Working with DataFrames</h2>
<p>The Pandas DataFrame type returned by the HTRC Feature Reader is very malleable. To work with the tokenlist that you retrieved earlier, three skills are particularily valuable:</p>
<ol>
<li>Selecting subsets by a condition</li>
<li>Slicing by named row index</li>
<li>Grouping and aggregating</li>
</ol>
<h3 id="selecting-subsets-of-a-dataframe-by-a-condition">Selecting Subsets of a DataFrame by a Condition</h3>
<p>Consider this example: <em>I only want to look at tokens that occur more than a hundred times in the book.</em></p>
<p>Remembering that the table-like output from the HTRC Feature Reader is a Pandas DataFrame, the way to pursue this goal is to learn to filter and subset DataFrames. Knowing how to do so is important for working with just the data that you need.</p>
<p>To subset individual rows of a DataFrame, you can provide a series of True/False values to the DataFrame, formatted in square brackets. When True, the DataFrame returns that row; when False, the row is excluded from what is returned.</p>
<p>To see this in context, first load a basic tokenlist without parts-of-speech or individual pages:</p>
<pre><code class="language-python">tl_simple = vol.tokenlist(pos=False, pages=False)
# .sample(5) returns five random words from the full result
tl_simple.sample(5)
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>count</th>
    </tr>
    <tr>
      <th>section</th>
      <th>token</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="5" valign="top">body</th>
      <th>halleluya</th>
      <td>4</td>
    </tr>
    <tr>
      <th>realty</th>
      <td>1</td>
    </tr>
    <tr>
      <th>addressed</th>
      <td>4</td>
    </tr>
    <tr>
      <th>win-</th>
      <td>1</td>
    </tr>
    <tr>
      <th>broke</th>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>



<p>To select just the relevant tokens, we need to look at each row and evaluate whether it matches the criteria that &quot;this token has a count greater than 100&quot;. Let&#39;s try to convert that requirement to code.</p>
<p>&quot;This token has a count&quot; means that we are concerned specifically with the &#39;count&#39; column, which can be singled out from the <code>tl</code> table with <code>tl[&#39;count&#39;]</code>. &quot;greater than 100&quot; is formalized as <code>&gt; 100</code>. Putting it together, try the following and see what you get:</p>
<pre><code class="language-python">tl_simple[&#39;count&#39;] &gt; 100
</code></pre>
<p>It is a DataFrame of True/False values. Each value indicates whether the &#39;count&#39; column in the corresponding row matches the criteria or not. We haven&#39;t selected a subset yet, we simply asked a question and were told for each row when the question was true or false.</p>
<blockquote>
<p>You may wonder why section and token are still seen, even though &#39;count&#39; was selected. These are part of the DataFrame <strong>index</strong>, so they&#39;re considered part of the information <em>about</em> that row rather than data <em>in</em> the row. You can convert the index to data columns with <code>reset_index()</code>. In this lesson we will keep the index intact, though there are advanced cases where there are benefits to resetting it.</p>
</blockquote>
<p>Armed with the True/False values of whether each token&#39;s &#39;count&#39; value is or isn&#39;t greater than 100, we can give those values to <code>tl_simple</code> in square brackets.</p>
<pre><code class="language-python">matches = tl_simple[&#39;count&#39;] &gt; 100
tl_simple[matches].sample(5)
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>count</th>
    </tr>
    <tr>
      <th>section</th>
      <th>token</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="5" valign="top">body</th>
      <th>they</th>
      <td>127</td>
    </tr>
    <tr>
      <th>have</th>
      <td>210</td>
    </tr>
    <tr>
      <th>The</th>
      <td>107</td>
    </tr>
    <tr>
      <th>about</th>
      <td>110</td>
    </tr>
    <tr>
      <th>,</th>
      <td>3258</td>
    </tr>
  </tbody>
</table>
</div>



<p>You can move the comparison straight into the square brackets, the more conventional equivalent of the above:</p>
<pre><code class="language-python">tl_simple[tl_simple[&#39;count&#39;] &gt; 100].sample(5)
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>count</th>
    </tr>
    <tr>
      <th>section</th>
      <th>token</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="5" valign="top">body</th>
      <th>not</th>
      <td>220</td>
    </tr>
    <tr>
      <th>had</th>
      <td>455</td>
    </tr>
    <tr>
      <th>have</th>
      <td>210</td>
    </tr>
    <tr>
      <th>,</th>
      <td>3258</td>
    </tr>
    <tr>
      <th>his</th>
      <td>206</td>
    </tr>
  </tbody>
</table>
</div>



<p>As might be expected, many of the tokens that occur very often are common words like &quot;she&quot; and &quot;and&quot;, as well as various punctuation.</p>
<p>Multiple conditions can be chained with <code>&amp;</code> (and) or <code>|</code> (or), using regular brackets so that Python knows the order of operations. For example, words with a count greater than 150 <em>and</em> a count less than 200 are selected in this way:</p>
<pre><code class="language-python">tl_simple[(tl_simple[&#39;count&#39;] &gt; 150) &amp; (tl_simple[&#39;count&#39;] &lt; 200)]
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>count</th>
    </tr>
    <tr>
      <th>section</th>
      <th>token</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="7" valign="top">body</th>
      <th>Mr.</th>
      <td>159</td>
    </tr>
    <tr>
      <th>be</th>
      <td>196</td>
    </tr>
    <tr>
      <th>but</th>
      <td>178</td>
    </tr>
    <tr>
      <th>do</th>
      <td>179</td>
    </tr>
    <tr>
      <th>is</th>
      <td>170</td>
    </tr>
    <tr>
      <th>on</th>
      <td>190</td>
    </tr>
    <tr>
      <th>â€”</th>
      <td>167</td>
    </tr>
  </tbody>
</table>
</div>



<h3 id="slicing-dataframes">Slicing DataFrames</h3>
<p>Above, subsets of the DataFrame were selected based on a matching criteria for columns. It is also possible to select a DataFrame subset by specifying the values of its index, a process called <strong>slicing</strong>. For example, you can ask, <em>&quot;give me all the verbs for pages 9-12&quot;</em>.</p>
<p>In the DataFrame returned by <code>vol.tokenlist()</code>, page, section, token, and POS were part of the index (try the command <code>tl.index.names</code> to confirm). One can think of an index as the margin content of an Excel spreadsheet: the letters along the top and numbers along the left side are the indices. A cell can be referred to as A1, A2, B1... In Pandas, however, you can name these, so instead of A, B, C, or 1,2,3, columns and rows can be referred to by more descriptive names. You can also have multiple levels, so you&#39;re not bound by the two-dimensions of a table format. With a multi-indexed DataFrame, you can ask for <code>Page=24,section=Body, ...</code>.</p>
<p>{% include figure.html filename=&quot;Excel.PNG&quot; caption=&quot;One can think of an index as the margin notations in Excel (i.e. 1,2,3... and A,B,C,..), except it can be named and can have multiple levels.&quot; %}</p>
<p>Slicing a DataFrame against a labelled index is done using <code>DataFrame.loc[]</code>. Try the following examples and see what is returned:</p>
<ul>
<li>Select information from page 17:<ul>
<li><code>tl.loc[(17),]</code></li>
</ul>
</li>
<li>Select &#39;body&#39; section of page 17:<ul>
<li><code>tl.loc[(17, &#39;body&#39;),]</code></li>
</ul>
</li>
<li>Select counts of the word &#39;Anne&#39; in the &#39;body&#39; section of page 17:<ul>
<li><code>tl.loc[(17, &#39;body&#39;, &#39;Anne&#39;),]</code></li>
</ul>
</li>
</ul>
<p>The levels of the index are specified in order, so in this case the first value refers to &#39;page&#39;, then &#39;section&#39;, and so on. To skip specifying anything for an index level -- that is, to select everything for that level -- <code>slice(None)</code> can be used as a placeholder:</p>
<ul>
<li>Select counts of the word &#39;Anne&#39; for all pages and all page sections<ul>
<li><code>tl.loc[(slice(None), slice(None), &quot;Anne&quot;),]</code></li>
</ul>
</li>
</ul>
<p>Finally, it is possible to select multiple labels for a level of the index, with a list of labels (i.e. <code>[&#39;label1&#39;, &#39;label2&#39;]</code>) or a sequence covering everything from one value to another (i.e. <code>slice(start, end)</code>):</p>
<ul>
<li>Select pages 37, 38, and 52<ul>
<li><code>tl.loc[([37, 38, 52]),]</code></li>
</ul>
</li>
<li>Select all pages from 37 to 40<ul>
<li><code>tl.loc[(slice(37, 40)),]</code></li>
</ul>
</li>
<li>Select counts for &#39;Anne&#39; or &#39;Hilary&#39; from all pages<ul>
<li><code>tl.loc[(slice(None), slice(None), [&quot;Anne&quot;, &quot;Hilary&quot;]),]</code></li>
</ul>
</li>
</ul>
<blockquote>
<p>The reason for the comma in <code>tl.loc[(...),]</code> is because columns can be selected in the same way after the comma. Pandas DataFrames can have a multiple-level index for columns, but the HTRC Feature Reader does not use this.</p>
</blockquote>
<p>Knowing how to slice, let&#39;s try to find the word &quot;CHAPTER&quot; in this book, and compare where that shows up to the token-per-page pattern previously plotted.</p>
<p>The token list we previously set to <code>tl</code> only included body text; to include headers and footers in a search for <code>CHAPTER</code> we&#39;ll grab a new tokenlist with <code>section=&#39;all&#39;</code> specified.</p>
<pre><code class="language-python">tl_all = vol.tokenlist(section=&#39;all&#39;)
chapter_pages = tl_all.loc[(slice(None), slice(None), &quot;CHAPTER&quot;),]
chapter_pages
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th>count</th>
    </tr>
    <tr>
      <th>page</th>
      <th>section</th>
      <th>token</th>
      <th>pos</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>19</th>
      <th>header</th>
      <th>CHAPTER</th>
      <th>NNP</th>
      <td>1</td>
    </tr>
    <tr>
      <th>35</th>
      <th>header</th>
      <th>CHAPTER</th>
      <th>NNP</th>
      <td>1</td>
    </tr>
    <tr>
      <th>56</th>
      <th>header</th>
      <th>CHAPTER</th>
      <th>NNP</th>
      <td>1</td>
    </tr>
    <tr>
      <th>73</th>
      <th>header</th>
      <th>CHAPTER</th>
      <th>NNP</th>
      <td>1</td>
    </tr>
    <tr>
      <th>91</th>
      <th>header</th>
      <th>CHAPTER</th>
      <th>NNP</th>
      <td>1</td>
    </tr>
    <tr>
      <th>115</th>
      <th>header</th>
      <th>CHAPTER</th>
      <th>NNP</th>
      <td>1</td>
    </tr>
    <tr>
      <th>141</th>
      <th>header</th>
      <th>CHAPTER</th>
      <th>NNP</th>
      <td>1</td>
    </tr>
    <tr>
      <th>158</th>
      <th>header</th>
      <th>CHAPTER</th>
      <th>NNP</th>
      <td>1</td>
    </tr>
    <tr>
      <th>174</th>
      <th>header</th>
      <th>CHAPTER</th>
      <th>NNP</th>
      <td>1</td>
    </tr>
    <tr>
      <th>193</th>
      <th>header</th>
      <th>CHAPTER</th>
      <th>NNP</th>
      <td>1</td>
    </tr>
    <tr>
      <th>217</th>
      <th>body</th>
      <th>CHAPTER</th>
      <th>NNP</th>
      <td>1</td>
    </tr>
    <tr>
      <th>231</th>
      <th>header</th>
      <th>CHAPTER</th>
      <th>NNP</th>
      <td>1</td>
    </tr>
    <tr>
      <th>246</th>
      <th>header</th>
      <th>CHAPTER</th>
      <th>NNP</th>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<p>Earlier, token counts were visualized using <code>tokens.plot()</code>, a built-in function of DataFrames that uses the Matplotlib visualization library.</p>
<p>We can add to the earlier visualization by using Matplotlib directly. Try the following code in a new cell, which goes through every page number in the earlier search for &#39;CHAPTER&#39; and adds a red vertical line at the place in the chart with <code>matplotlib.pyplot.axvline()</code>:</p>
<pre><code class="language-python"># Get just the page numbers from the search for &quot;CHAPTER&quot;
page_numbers = chapter_pages.index.get_level_values(&#39;page&#39;)

# Visualize the tokens-per-page from before
tokens.plot()

# Add vertical lines for pages with &quot;CHAPTER&quot;
import matplotlib.pyplot as plt
for page_number in page_numbers:
    plt.axvline(x=page_number, color=&#39;red&#39;)
</code></pre>
<p>{% include figure.html filename=&quot;draft_41_0.png&quot; caption=&quot;Output graph.&quot; %}</p>
<blockquote>
<p>Advanced: Though slicing with <code>loc</code> is more common when working with the index, it is possible to create a True/False list from an index to select rows as we did earlier. Here&#39;s an advanced example that grabs the &#39;token&#39; part of the index and, using the <code>isalpha()</code> string method that Pandas provides, filters to fully alphabetical words.</p>
</blockquote>
<pre><code>token_idx = tl.index.get_level_values(&quot;token&quot;)
tl[token_idx.str.isalpha()]
</code></pre>
<p>Readers familiar with regular expressions (see <a href="/lessons/understanding-regular-expressions">Understanding Regular Expressions</a> by Doug Knox) can adapt this example for even more robust selection using the <code>contains()</code> string method.</p>
<h2 id="sorting-dataframes">Sorting DataFrames</h2>
<p>A DataFrame can be sorted with <code>DataFrame.sort_values()</code>, specifying the column to sort by as the first argument. By default, sorting is done in ascending order:</p>
<pre><code class="language-python">tl_simple.sort_values(&#39;count&#39;).head()
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>count</th>
    </tr>
    <tr>
      <th>section</th>
      <th>token</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="5" valign="top">body</th>
      <th>gratified</th>
      <td>1</td>
    </tr>
    <tr>
      <th>reminding</th>
      <td>1</td>
    </tr>
    <tr>
      <th>dome</th>
      <td>1</td>
    </tr>
    <tr>
      <th>remembering</th>
      <td>1</td>
    </tr>
    <tr>
      <th>remains</th>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<p>Descending order is possible with the argument <code>ascending=False</code>, which puts the most common tokens at the top. For example:</p>
<pre><code class="language-python">tl_simple.sort_values(&#39;count&#39;, ascending=False).head()
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>count</th>
    </tr>
    <tr>
      <th>section</th>
      <th>token</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="5" valign="top">body</th>
      <th>,</th>
      <td>3258</td>
    </tr>
    <tr>
      <th>"</th>
      <td>1670</td>
    </tr>
    <tr>
      <th>the</th>
      <td>1565</td>
    </tr>
    <tr>
      <th>.</th>
      <td>1532</td>
    </tr>
    <tr>
      <th>and</th>
      <td>1252</td>
    </tr>
  </tbody>
</table>
</div>



<p>The most common tokens are &#39;the&#39; and &#39;and&#39;, alongside punctuation.</p>
<p><em>Exercise: Try to retrieve the five most-common tokens used as a noun (&#39;NNP&#39;) or a plural noun (&#39;NNS&#39;) in the book</em>. You will have to get a new tokenlist, without pages but with parts-of-speech, then slice by the criteria, sort, and output the first five rows. (<a href="https://gist.github.com/organisciak/163e59ea6cf71c3cd12de410d075567c">Solution</a>)</p>
<h3 id="grouping-dataframes">Grouping DataFrames</h3>
<p>Up to this point, the token count DataFrames have been subsetted, but not modified from the way they were returned by the HTRC Feature Reader. There are many cases where one may want to perform aggregation or transformation based on subsets of data. To do this, Pandas supports the &#39;split-apply-combine&#39; pattern (Wickham 2011).</p>
<p>Split-apply-combine refers to the process of dividing a dataset into groups (<em>split</em>), performing some activity for each of those groups (<em>apply</em>), and joining the new groups back together into a single DataFrame (<em>combine</em>).</p>
<p>{% include figure.html filename=&quot;split-apply-combine.png&quot; caption=&quot;Graph demonstrating Split-Apply-Combine.&quot; %}</p>
<p>{% include figure.html filename=&quot;example-split-apply-combine.png&quot; caption=&quot;Example of Split-Apply-Combine, averaging movie grosses by director.&quot; %}</p>
<p>Split-apply-combine processes are supported on DataFrames with <code>groupby()</code>, which tells Pandas to split by some criteria. From there, it is possible to apply some change to each group individually, after which Pandas combines the affected groups into a single DataFrame again.</p>
<p>Try the following, can you tell what happens?</p>
<pre><code>tl.groupby(level=[&quot;pos&quot;]).sum()
</code></pre>
<p>The output is a count of how often each part-of-speech tag (&quot;pos&quot;) occurs in the entire book.</p>
<ul>
<li><em>Split</em> with <code>groupby()</code>: We took the token count dataframe that is set to <code>tl</code> and grouped by the part-of-speech (<code>pos</code>) level of the index. This means that rather than thinking in terms of rows, Pandas is now thinking of the <code>tl</code> DataFrame as a series of smaller groups, the groups selected by a common value for part of speech. So, all the personal pronouns (&quot;PRP&quot;) are in one group, and all the adverbs (&quot;RB&quot;) are in another, and so on.</li>
<li><em>Apply</em> with <code>sum()</code>: These groups were sent to an apply function, <code>sum()</code>. Sum is an aggregation function, so it sums all the information in the &#39;count&#39; column for each group. For example, all the rows of data in the adverb group are summed up into a single count of all adverbs.</li>
<li><em>Combine</em>: The combine step is implicit: the DataFrame knows from the <code>groupby</code> pattern to take everything that the apply function gives back (in the case of &#39;sum&#39;, just one row for every group) and stick it together.</li>
</ul>
<p><code>sum()</code> is one of many convenient functions <a href="http://pandas.pydata.org/pandas-docs/stable/groupby.html">built-in</a> to Pandas. Other useful functions are <code>mean()</code>, <code>count()</code>, <code>max()</code>. It is also possible to send your groups to any function that you write with <code>apply()</code>.</p>
<blockquote>
<p>groupby can be used on data columns or an index. To run against an index, use <code>level=[index_level_name]</code> as above. To group against columns, use <code>by=[column_name]</code>.</p>
</blockquote>
<p>Below are some examples of grouping token counts.</p>
<ul>
<li>Find most common tokens in the entire volume (sorting by most to least occurrences)<ul>
<li><code>tl.groupby(level=&quot;token&quot;).sum().sort_values(&quot;count&quot;, ascending=False)</code></li>
</ul>
</li>
<li>Count how many pages each token/pos combination occurs on<ul>
<li><code>tl.groupby(level=[&quot;token&quot;, &quot;pos&quot;]).count()</code></li>
</ul>
</li>
</ul>
<p>Remember from earlier that certain information can be called by sending arguments to <code>vol.tokenlist()</code>, so you don&#39;t always have to do the grouping yourself.</p>
<p>With <code>sum</code>, the data is being reduced: only one row is left for each group. It is also possible to &#39;transform&#39; a group, where the same number of rows are returned. This is useful if processing is necessary based on the group statistics, such as percentages. Here is an advanced example of transformation, a <a href="https://web.archive.org/web/20161108211721/https://porganized.com/2016/03/09/term-weighting-for-humanists/">TF*IDF</a> function. TF*IDF weighs a token&#39;s value to a document based on how common it is. In this case, it highlights words that are notable for a page but not the entire book.</p>
<pre><code class="language-python">from numpy import log
def tfidf(x):
    return x * log(1+vol.page_count / x.count())
# Will take a few seconds to run, depending on your system
idf_scores = tl.groupby(level=[&quot;token&quot;]).transform(tfidf)
idf_scores[1000:1100:30]
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th>count</th>
    </tr>
    <tr>
      <th>page</th>
      <th>section</th>
      <th>token</th>
      <th>pos</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>24</th>
      <th>body</th>
      <th>years</th>
      <th>NNS</th>
      <td>2.315830</td>
    </tr>
    <tr>
      <th rowspan="3" valign="top">25</th>
      <th rowspan="3" valign="top">body</th>
      <th>asked</th>
      <th>VBD</th>
      <td>1.730605</td>
    </tr>
    <tr>
      <th>him</th>
      <th>PRP</th>
      <td>2.994040</td>
    </tr>
    <tr>
      <th>n't</th>
      <th>RB</th>
      <td>1.250162</td>
    </tr>
  </tbody>
</table>
</div>



<p>Compare the parts of the function given to <code>transform()</code> with the equation:</p>
<p>$$ IDF_w = log(1 + \frac{N}{df_w}) $$</p>
<p>N is the total number of pages. Document frequency, $$df_w$$, is &#39;how many pages (docs) does the word occur on?&#39; That is the <code>x.count()</code>. Can you modify the above to use corpus frequency, which is &#39;how many times does the word occur overall in the corpus (i.e. across all pages)?&#39; You&#39;d want to add everything up.</p>
<h1 id="more-features-in-the-htrc-extracted-features-dataset">More Features in the HTRC Extracted Features Dataset</h1>
<p>So far we have mainly used token-counting features, accessed through <code>Volume.tokenlist()</code>. The HTRC Extracted Features Dataset provides more features at the volume level. Here are other features that are available to Volume objects. Try them on <code>vol</code> and see what the output is:</p>
<ul>
<li><code>vol.line_counts()</code>: How many vertically spaced lines of text, a measure related to the phyical format of the page.</li>
<li><code>vol.sentence_counts()</code>: How many sentences of text: a measure related to the content on a page.</li>
<li><code>vol.empty_line_counts()</code>: How many larger vertical spaces are there on the page between lines of text? In many cases, this can be used as a proxy for paragraph count. This is based on what software was used to OCR so there are inconsistencies: not all scans in the HathiTrust are OCR&#39;d identically.</li>
<li><code>vol.begin_line_chars()</code>, <code>vol.end_line_chars()</code>: The count of different characters along the left-most and right-most sides of a page. This can tell you about what kind of page it is: for example, a table of contents might have a lot of numbers or roman numerals at the end of each line</li>
</ul>
<p>Earlier, we saw that the number of words on a page gave some indication of whether it was a page of the story or a different kind of page (chapter, front matter, etc). We can see that line count is another contextual &#39;hint&#39;:</p>
<pre><code class="language-python">line_counts = vol.line_counts()
plt.plot(line_counts)
</code></pre>
<p>{% include figure.html filename=&quot;draft_52_1.png&quot; caption=&quot;Output graph.&quot; %}</p>
<p>The majority of pages have 20-25 lines, confirmable with a histogram: <code>plt.hist(line_counts)</code>. This is likely what a full page of text looks like in this book. A scholar trying to focus on patterns only in the text and comfortable missing a few short pages might choose to filter to just these pages.</p>
<h2 id="page-level-features">Page-Level Features</h2>
<p>If you open the raw dataset file for a HTRC EF volume on your computer, you may notice that features are provided for each page. While this lesson has focused on volumes, most of the features that we have seen can be accessed for a single page; e.g. <code>Page.tokenlist()</code> instead of <code>Volume.tokenlist()</code>. The methods to access the features are named the same, with the exception that <code>line_count</code>, <code>empty_line_count</code>, and <code>sentence_count</code> are not pluralized.</p>
<p>Like iterating over <code>FeatureReader.volumes()</code> to get Volume objects, it is possible to iterate across pages with <code>Volume.pages()</code>.</p>
<h1 id="next-steps">Next Steps</h1>
<p>Now that you know the basics of the HTRC Feature Reader, you can learn more about the <a href="https://analytics.hathitrust.org/features">Extracted Features dataset</a>. The <a href="https://github.com/htrc/htrc-feature-reader/blob/master/README.ipynb">Feature Reader home page</a> contains a lesson similar to this one but for more advanced users (that&#39;s you now!), and the <a href="http://htrc.github.io/htrc-feature-reader/htrc_features/feature_reader.m.html">code documentation</a> gives exact information about what types of information can be called.</p>
<p>Underwood (2015) has released <a href="https://analytics.hathitrust.org/genre">genre classifications of public-domain texts in the HTRC EF Dataset</a>, comprised of fiction, poetry, and drama. Though many historians will be interested in other corners of the dataset, fiction is a good place to tinker with text mining ideas because of its expressiveness and relative format consistency.</p>
<p>Finally, the repository for the HTRC Feature Reader has <a href="https://github.com/htrc/htrc-feature-reader/tree/master/examples">advanced tutorial notebooks</a> showing how to use the library further. One such tutorial shows how to <a href="https://github.com/htrc/htrc-feature-reader/blob/master/examples/Within-Book%20Sentiment%20Trends.ipynb">derive &#39;plot arcs&#39; for a text</a>, a process popularized by Jockers (2015).</p>
<p>{% include figure.html filename=&quot;plot-arc.png&quot; caption=&quot;Plot Arc Example.&quot; %}</p>
<h1 id="references">References</h1>
<p>Boris Capitanu, Ted Underwood, Peter Organisciak, Timothy Cole, Maria Janina Sarol, J. Stephen Downie (2016). The HathiTrust Research Center Extracted Feature Dataset (1.0) [Dataset]. HathiTrust Research Center, <a href="https://doi.org/10.13012/J8X63JT3">https://doi.org/10.13012/J8X63JT3</a>.</p>
<p>Chris Forster. &quot;A Walk Through the Metadata: Gender in the HathiTrust Dataset.&quot; Blog. <a href="http://cforster.com/2015/09/gender-in-hathitrust-dataset/">http://cforster.com/2015/09/gender-in-hathitrust-dataset/</a>.</p>
<p>Matthew L. Jockers (Feb 2015). &quot;Revealing Sentiment and Plot Arcs with the Syuzhet Package&quot;. <em>Matthew L. Jockers</em>. Blog. <a href="http://www.matthewjockers.net/2015/02/02/syuzhet/">http://www.matthewjockers.net/2015/02/02/syuzhet/</a>.</p>
<p>Peter Organisciak, Loretta Auvil, J. Stephen Downie (2015). â€œRemembering books: A within-book topic mapping technique.â€ Digital Humanities 2015. Sydney, Australia.</p>
<p>StÃ©fan Sinclair &amp; Geoffrey Rockwell (2016). &quot;The Art of Literary Text Analysis.&quot; Github.com. Commit b04bc18. <a href="https://github.com/sgsinclair/alta">https://github.com/sgsinclair/alta</a>.</p>
<p>William J. Turkel and Adam Crymble (2012). &quot;Counting Word Frequencies with Python&quot;. The Programming Historian. /lessons/counting-frequencies.</p>
<p>Ted Underwood (2014): Understanding Genre in a Collection of a Million Volumes, Interim Report. figshare.
<a href="https://doi.org/10.6084/m9.figshare.1281251.v1">https://doi.org/10.6084/m9.figshare.1281251.v1</a>.</p>
<p>Ted Underwood, Boris Capitanu, Peter Organisciak, Sayan Bhattacharyya, Loretta Auvil, Colleen Fallaw, J. Stephen Downie (2015). &quot;Word Frequencies in English-Language Literature, 1700-1922&quot; (0.2) [Dataset]. <em>HathiTrust Research Center</em>. <a href="https://doi.org/10.13012/J8JW8BSJ">https://doi.org/10.13012/J8JW8BSJ</a>.</p>
<p>Hadley Wickham (2011). &quot;The split-apply-combine strategy for data analysis&quot;. <em>Journal of Statistical Software</em>, 40(1), 1-29.</p>
<h1 id="appendix-downloading-custom-files-via-rsync">Appendix: Downloading custom files via rsync</h1>
<p>The full HTRC Extracted Features dataset is accessible using <em>rsync</em>, a Unix command line program for syncing files. It is already preinstalled on Linux or Mac OS. Windows users need to use <em>rsync</em> by downloading a program such as <a href="https://cygwin.com/">Cygwin</a>, which provides a Unix-like command line environment in Windows.</p>
<p>To download all <em>4 TB</em> comprising the EF dataset, you can use this command (be aware the full transfer will take a very long time):</p>
<pre><code class="language-bash">rsync -rv data.analytics.hathitrust.org::features/ .
</code></pre>
<p>This command recurses (the <code>-r</code> flag) through all the folders on the HTRC server, and syncs all the files to a location on your system; in this case the <code>.</code> at the end means &quot;the current folder&quot;. The <code>-v</code> flag means <code>--verbose</code>, which tells rsync to show you more information.</p>
<p>It is possible to sync individual files by specifying a full file path. Files are organized in a <a href="https://wiki.ucop.edu/display/Curation/PairTree">PairTree structure</a>, meaning that you can find an exact dataset file from a volume&#39;s HathiTrust id. The HTRC Feature Reader has a tools and instructions for <a href="https://github.com/htrc/htrc-feature-reader/blob/master/examples/ID_to_Rsync_Link.ipynb">getting the path for a volume</a>. A list of all file paths is available:</p>
<pre><code class="language-bash">rsync -azv data.analytics.hathitrust.org::features/listing/htrc-ef-all-files.txt .
</code></pre>
<p>Finally, it is possible to download many files from a list. To try, we&#39;ve put together lists for public-domain <a href="http://data.analytics.hathitrust.org/genre/fiction_paths.txt">fiction</a>, <a href="http://data.analytics.hathitrust.org/genre/drama_paths.txt">drama</a>, and <a href="http://data.analytics.hathitrust.org/genre/poetry_paths.txt">poetry</a> (Underwood 2014). For example:</p>
<pre><code class="language-bash">rsync -azv --files-from=fiction_paths.txt data.analytics.hathitrust.org::features/ .
</code></pre>
<!-- HTML_TAG_END -->

<script type="application/json" data-type="svelte-data" data-url="text-mining-with-extracted-features/raw.json">{"status":200,"statusText":"","headers":{"content-type":"application/json; charset=utf-8"},"body":"{\"metadata\":{\"title\":\"Text Mining in Python through the HTRC Feature Reader\",\"authors\":[\"Peter Organisciak\",\"Boris Capitanu\"],\"date\":\"2016-11-22T00:00:00.000Z\",\"reviewers\":[\"StÃ©fan Sinclair\",\"Catherine DeRose\"],\"editors\":[\"Ian Milligan\"],\"layout\":\"lesson\",\"activity\":\"analyzing\",\"topics\":[\"distant-reading\"],\"difficulty\":3,\"review-ticket\":\"https:\u002F\u002Fgithub.com\u002Fprogramminghistorian\u002Fph-submissions\u002Fissues\u002F29\",\"abstract\":\"Explains how to use Python to summarize and visualize data on millions of texts from the HathiTrust Research Center's Extracted Features dataset.\\n\",\"redirect_from\":\"\u002Flessons\u002Ftext-mining-with-extracted-features\",\"mathjax\":true,\"avatar_alt\":\"A book inside a torn case\",\"doi\":\"10.46430\u002Fphen0058\"},\"html_body\":\"\u003Cp\u003E{% include toc.html %}\u003C\u002Fp\u003E\\n\u003Cp\u003ESummary: \u003Cem\u003EWe introduce a toolkit for working with the 13.6 million volume Extracted Features Dataset from the HathiTrust Research Center. You will learn how to peer at the words and trends of any book in the collection, while developing broadly useful Python data analysis skills.\u003C\u002Fem\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003EThe \u003Ca href=\\\"https:\u002F\u002Fwww.hathitrust.org\\\"\u003EHathiTrust\u003C\u002Fa\u003E holds nearly 15 million digitized volumes from libraries around the world. In addition to their individual value, these works in aggregate are extremely valuable for historians. Spanning many centuries and genres, they offer a way to learn about large-scale trends in history and culture, as well as evidence for changes in language or even the structure of the book. To simplify access to this collection the HathiTrust Research Center (HTRC) has released the Extracted Features dataset (Capitanu et al. 2015): a dataset that provides quantitative information describing every page of every volume in the collection.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn this lesson, we introduce the HTRC Feature Reader, a library for working with the HTRC Extracted Features dataset using the Python programming language. The HTRC Feature Reader is structured to support work using popular data science libraries, particularly Pandas. Pandas provides simple structures for holding data and powerful ways to interact with it. The HTRC Feature Reader uses these data structures, so learning how to use it will also cover general data analysis skills in Python.\u003C\u002Fp\u003E\\n\u003Cp\u003EToday, you&#39;ll learn:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003EHow to work with \u003Cem\u003Enotebooks\u003C\u002Fem\u003E, an interactive environment for data science in Python;\u003C\u002Fli\u003E\\n\u003Cli\u003EMethods to read and visualize text data for millions of books with the HTRC Feature Reader; and\u003C\u002Fli\u003E\\n\u003Cli\u003EData malleability, the skills to select, slice, and summarize extracted features data using the flexible &quot;DataFrame&quot; structure.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Ch2 id=\\\"background\\\"\u003EBackground\u003C\u002Fh2\u003E\\n\u003Cp\u003EThe \u003Cstrong\u003EHathiTrust Research Center\u003C\u002Fstrong\u003E (\u003Cstrong\u003EHTRC\u003C\u002Fstrong\u003E) is the research arm of the HathiTrust, tasked with supporting research usage of the works held by the HathiTrust. Particularly, this support involves mediating large-scale access to materials in a non-consumptive manner, which aims to allow research over a work without enabling that work to be traditionally enjoyed or read by a human reader.  Huge digital collections can be of public benefit by allowing scholars to discover insights about history and culture, and the non-consumptive model allows for these uses to be sought within the restrictions of intellectual property law.\u003C\u002Fp\u003E\\n\u003Cp\u003EAs part of its mission, the HTRC has released the \u003Cstrong\u003EExtracted Features\u003C\u002Fstrong\u003E (\u003Cstrong\u003EEF\u003C\u002Fstrong\u003E) dataset containing features derived for every page of 13.6 million &#39;volumes&#39; (a generalized term referring to the different types of materials in the HathiTrust collection, of which books are the most prevalent type).\u003C\u002Fp\u003E\\n\u003Cp\u003EWhat is a feature? A \u003Cstrong\u003Efeature\u003C\u002Fstrong\u003E is a quantifiable marker of something measurable, a datum. A computer cannot understand the meaning of a sentence implicitly, but it can understand the counts of various words and word forms, or the presence or absence of stylistic markers, from which it can be trained to better understand text. Many text features are non-consumptive in that they don&#39;t retain enough information to reconstruct the book text.\u003C\u002Fp\u003E\\n\u003Cp\u003ENot all features are useful, and not all algorithms use the same features. With the HTRC EF Dataset, we have tried to include the most generally useful features, as well as adapt to scholarly needs. We include per-page information such as counts of words tagged by part of speech (e.g. \u003Cem\u003Ehow many times does the word \u003Ccode\u003Ejaguar\u003C\u002Fcode\u003E appear as a lowercase noun on this page\u003C\u002Fem\u003E), line and sentence counts, and counts of characters at the leftmost and rightmost sides of a page. No positional information is provided, so the data would not specify if &#39;brown&#39; is followed by &#39;dog&#39;, though the information is shared for every single page, so you can at least infer how often &#39;brown&#39; and &#39;dog&#39; occurred in the same general vicinity within a text.\u003C\u002Fp\u003E\\n\u003Cp\u003EFreely accessible and preprocessed, the Extracted Features dataset offers a great entry point to programmatic text analysis and text mining. To further simplify beginner usage, the HTRC has released the HTRC Feature Reader. The \u003Cstrong\u003EHTRC Feature Reader\u003C\u002Fstrong\u003E scaffolds use of the dataset with the Python programming language.\u003C\u002Fp\u003E\\n\u003Cp\u003EThis tutorial teaches the fundamentals of using the Extracted Features dataset with the HTRC Feature Reader. The HTRC Feature Reader is designed to make use of data structures from the most popular scientific tools in Python, so the skills taught here will apply to other settings of data analysis. In this way, the Extracted Features dataset is a particularly good use case for learning more general text analysis skills. We will look at data structures for holding text, patterns for querying and filtering that information, and ways to summarize, group, and visualize the data.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"possibilities\\\"\u003EPossibilities\u003C\u002Fh2\u003E\\n\u003Cp\u003EThough it is relatively new, the Extracted Features dataset is already seeing use by scholars, as seen on a \u003Ca href=\\\"https:\u002F\u002Fwiki.htrc.illinois.edu\u002Fdisplay\u002FCOM\u002FExtracted+Features+in+the+Wild\\\"\u003Epage collected by the HTRC\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Ca href=\\\"https:\u002F\u002Fdoi.org\u002F10.6084\u002Fm9.figshare.1279201\\\"\u003EUnderwood\u003C\u002Fa\u003E leveraged the features for identifying genres, such as fiction, poetry, and drama (2014). Associated with this work, he has released a dataset of 178k books classified by genre alongside genre-specific word counts (\u003Ca href=\\\"https:\u002F\u002Fdoi.org\u002F10.13012\u002FJ8JW8BSJ\\\"\u003EUnderwood 2015\u003C\u002Fa\u003E).\u003C\u002Fp\u003E\\n\u003Cp\u003EThe Underwood subset of the Extracted Features dataset was used by Forster (2015) to \u003Ca href=\\\"http:\u002F\u002Fcforster.com\u002F2015\u002F09\u002Fgender-in-hathitrust-dataset\u002F\\\"\u003Eobserving gender in literature\u003C\u002Fa\u003E, illustrating the decline of woman authors through the 19th century.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe Extracted Features dataset also underlies higher-level analytic tools. \u003Ca href=\\\"http:\u002F\u002Fmimno.infosci.cornell.edu\u002Fwordsim\u002Fnearest.html\\\"\u003EMimno\u003C\u002Fa\u003E processed word co-occurrence tables per year, allowing others to view how correlations between topics change over time (2014). The \u003Ca href=\\\"https:\u002F\u002Fanalytics.hathitrust.org\u002Fbookworm\\\"\u003EHT Bookworm\u003C\u002Fa\u003E project has developed an API and visualization tools to support exploration of trends within the HathiTrust collection across various classes, genres, and languages. Finally, we have developed an approach to \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Forganisciak\u002Fhtrc-book-models\\\"\u003Ewithin-book topic modelling\u003C\u002Fa\u003E which functions as a mnemonic accompaniment to a previously-read book (Organisciak 2014).\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"suggested-prior-skills\\\"\u003ESuggested Prior Skills\u003C\u002Fh2\u003E\\n\u003Cp\u003EThis lesson provides a gentle but technical introduction to text analysis in Python with the HTRC Feature Reader. Most of the code is provided, but is most useful if you are comfortable tinkering with it and seeing how outputs change when you do.\u003C\u002Fp\u003E\\n\u003Cp\u003EWe recommend a baseline knowledge of Python conventions, which can be learned with Turkel and Crymble&#39;s \u003Ca href=\\\"\u002Flessons\u002Fintroduction-and-installation\\\"\u003Eseries of Python lessons\u003C\u002Fa\u003E on Programming Historian.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe skills taught here are focused on flexibly accessing and working with already-computed text features. For a better understanding of the process of deriving word features, Programming Historian provides a lesson on \u003Ca href=\\\"\u002Flessons\u002Fcounting-frequencies\\\"\u003ECounting Frequencies\u003C\u002Fa\u003E, by Turkel and Crymble.\u003C\u002Fp\u003E\\n\u003Cp\u003EA more detailed look at text analysis with Python is provided in the \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fsgsinclair\u002Falta\u002Fblob\u002Fmaster\u002Fipynb\u002FArtOfLiteraryTextAnalysis.ipynb\\\"\u003EArt of Literary Text Analysis\u003C\u002Fa\u003E (Sinclair). The Art of Literary Text Analysis (ALTA) provides a deeper introduction to foundation Python skills, as well as introduces further text analytics concepts to accompany the skills we cover in this lesson. This includes lessons on extracting features (\u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fsgsinclair\u002Falta\u002Fblob\u002Fmaster\u002Fipynb\u002FNltk.ipynb\\\"\u003Etokenization\u003C\u002Fa\u003E, \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fsgsinclair\u002Falta\u002Fblob\u002Fmaster\u002Fipynb\u002FRepeatingPhrases.ipynb\\\"\u003Ecollocations\u003C\u002Fa\u003E), and \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fsgsinclair\u002Falta\u002Fblob\u002Fmaster\u002Fipynb\u002FGettingGraphical.ipynb\\\"\u003Evisualizing trends\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"download-the-lesson-files\\\"\u003EDownload the Lesson Files\u003C\u002Fh1\u003E\\n\u003Cp\u003ETo follow along, download \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fprogramminghistorian\u002Fph-submissions\u002Fraw\u002Fgh-pages\u002Fassets\u002Fextracted-features-lesson_files.zip\\\"\u003Elesson_files.zip\u003C\u002Fa\u003E and unzip it to any directory you choose.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe lesson files include a sample of files from the HTRC Extracted Features dataset. After you learn to use the feature data in this lesson, you may want to work with the entirety of the dataset. The details on how to do this are described in \u003Ca href=\\\"#appendix-downloading-custom-files-via-rsync\\\"\u003EAppendix: rsync\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"installation\\\"\u003EInstallation\u003C\u002Fh2\u003E\\n\u003Cp\u003EFor this lesson, you need to install the HTRC Feature Reader library for Python alongside the data science libraries that it depends on.\u003C\u002Fp\u003E\\n\u003Cp\u003EFor ease, this lesson will focus on installing Python through a scientific distribution called Anaconda. Anaconda is an easy-to-install Python distribution that already includes most of the dependencies for the HTRC Feature Reader.\u003C\u002Fp\u003E\\n\u003Cp\u003ETo install Anaconda, download the installer for your system from the \u003Ca href=\\\"https:\u002F\u002Fwww.continuum.io\\\"\u003EAnaconda download page\u003C\u002Fa\u003E and follow their instructions for installation of either the Windows 64-bit Graphical Installer or the Mac OS X 64-bit Graphical Installer. You can choose either version of Python for this lesson. If you have followed earlier lessons on Python at the \u003Cem\u003EProgramming Historian\u003C\u002Fem\u003E, you are using Python 2, but the HTRC Feature Reader also supports Python 3.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;conda-install.PNG&quot; caption=&quot;Conda Install&quot; %}\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"installing-the-htrc-feature-reader\\\"\u003EInstalling the HTRC Feature Reader\u003C\u002Fh3\u003E\\n\u003Cp\u003EThe HTRC Feature Reader can be installed by command line. First open a terminal application:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003E\u003Cem\u003EWindows\u003C\u002Fem\u003E: Open &#39;Command Prompt&#39; from the Start Menu and type: \u003Ccode\u003Eactivate\u003C\u002Fcode\u003E.\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cem\u003EMac OS\u002FLinux\u003C\u002Fem\u003E: Open &#39;Terminal&#39; from Applications and type \u003Ccode\u003Esource activate\u003C\u002Fcode\u003E.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003EIf Anaconda was properly installed, you should see something similar to this:\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;activating-env.PNG&quot; caption=&quot;Activating the default Anaconda environment.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003ENow, you need to type one command:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Econda install -c htrc htrc-feature-reader\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThis command installs the HTRC Feature Reader and its necessary dependencies. We specify \u003Ccode\u003E-c htrc\u003C\u002Fcode\u003E so the installation command knows to find the library from the \u003Ccode\u003Ehtrc\u003C\u002Fcode\u003E organization.\u003C\u002Fp\u003E\\n\u003Cp\u003EThat&#39;s it! At this point you have everything necessary to start reading HTRC Feature Reader files.\u003C\u002Fp\u003E\\n\u003Cblockquote\u003E\\n\u003Cp\u003E\u003Cem\u003Epsst\u003C\u002Fem\u003E, advanced users: You can install the HTRC Feature Reader \u003Cem\u003Ewithout\u003C\u002Fem\u003E Anaconda with \u003Ccode\u003Epip install htrc-feature-reader\u003C\u002Fcode\u003E, though for this lesson you&#39;ll need to install two additional libraries \u003Ccode\u003Epip install matplotlib jupyter\u003C\u002Fcode\u003E. Also, note that not all manual installations are alike because of hard-to-configure system optimizations: this is why we recommend Anaconda. If you think your code is going slow, you should check that Numpy has access to \u003Ca href=\\\"http:\u002F\u002Fstackoverflow.com\u002Fa\u002F19350234\u002F233577\\\"\u003EBLAS and LAPACK libraries\u003C\u002Fa\u003E and install \u003Ca href=\\\"http:\u002F\u002Fpandas.pydata.org\u002Fpandas-docs\u002Fversion\u002F0.15.2\u002Finstall.html#recommended-dependencies\\\"\u003EPandas recommended packages\u003C\u002Fa\u003E. The rest is up to you, advanced user!\u003C\u002Fp\u003E\\n\u003C\u002Fblockquote\u003E\\n\u003Ch2 id=\\\"start-a-notebook\\\"\u003EStart a Notebook\u003C\u002Fh2\u003E\\n\u003Cp\u003EUsing Python the traditional way -- writing a script to a file and running it -- can become clunky for text analysis, where the ability to look at and interact with data is invaluable.\\nThis lesson uses an alternative approach: Jupyter notebooks.\u003C\u002Fp\u003E\\n\u003Cp\u003EJupyter gives you an interactive version of Python (called IPython) that you can access in a &quot;notebook&quot; format in your web browser. This format has many benefits. The interactivity means that you don&#39;t need to re-run an entire script each time: you can run or re-run blocks of code as you go along, without losing your enviroment (i.e. the variables and code that are already loaded). The notebook format makes it easier to examine bits of information as you go along, and allows for text blocks to intersperse a narrative.\u003C\u002Fp\u003E\\n\u003Cp\u003EJupyter was installed alongside Anaconda in the previous section, so it should be available to load now.\u003C\u002Fp\u003E\\n\u003Cp\u003EFrom the Start Menu (Windows) or Applications directory (Mac OS), open &quot;Jupyter notebook&quot;. This will start Jupyter on your computer and open a browser window. Keep the console window in the background, the browser is where the magic happens.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;open-notebook.PNG&quot; caption=&quot;Opening Jupyter on Windows&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EIf your web browser does not open automatically, Jupyter can be accessed by going to the address &quot;localhost:8888&quot; - or a different port number, which is noted in the console (&quot;The Jupyter Notebook is running at...&quot;):\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;notebook-start.png&quot; caption=&quot;A freshly started Jupyter notebook instance.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EJupyter is now showing a directory structure from your home folder. Navigate to the lesson folder where you unzipped \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fprogramminghistorian\u002Fph-submissions\u002Fraw\u002Fgh-pages\u002Fassets\u002Fextracted-features-lesson_files.zip\\\"\u003Elesson_files.zip\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn the lesson folder, open \u003Ccode\u003EStart Here.pynb\u003C\u002Fcode\u003E: your first notebook!\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;notebook-hello-world.png&quot; caption=&quot;Hello world in a notebook&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EHere there are instructions for editing a cell of text or code, and running it. Try editing and running a cell, and notice that it only affects itself. Here are a few tips for using the notebook as the lesson continues:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003ENew cells are created with the \u003Ci class=\\\"fa-plus fa\\\"\u003E Plus\u003C\u002Fi\u003E button in the toolbar. When not editing, this can be done by pressing &#39;b&#39; on your keyboard.\u003C\u002Fli\u003E\\n\u003Cli\u003ENew cells are &quot;code&quot; cells by default, but can be changed to &quot;Markdown&quot; (a type of text input) in a dropdown menu on the toolbar. In edit mode, you can paste in code from this lesson or type it yourself.\u003C\u002Fli\u003E\\n\u003Cli\u003ESwitching a cell to edit mode is done by pressing Enter.\u003C\u002Fli\u003E\\n\u003Cli\u003ERunning a cell is done by clicking \u003Ci class=\\\"fa-step-forward fa\\\"\u003E Play\u003C\u002Fi\u003E in the toolbar, or with \u003Ccode\u003ECtrl+Enter\u003C\u002Fcode\u003E (\u003Ccode\u003ECtrl+Return\u003C\u002Fcode\u003E on Mac OS). To run a cell and immediately move forward, use \u003Ccode\u003EShift+Enter\u003C\u002Fcode\u003E instead.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cblockquote\u003E\\n\u003Cp\u003EAn example of a full-fledged notebook is included with the lesson files in \u003Ccode\u003Eexample\u002FLesson Draft.ipynb\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003C\u002Fblockquote\u003E\\n\u003Cp\u003EIn this notebook, it&#39;s time to give the HTRC Feature Reader a try. When it is time to try some code, start a new cell with \u003Ci class=\\\"fa-plus fa\\\"\u003E Plus\u003C\u002Fi\u003E, and run the code with \u003Ci class=\\\"fa-step-forward fa\\\"\u003E Play\u003C\u002Fi\u003E. Before continuing, click on the title to change it to something more descriptive than &quot;Start Here&quot;.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"reading-your-first-volume\\\"\u003EReading your First Volume\u003C\u002Fh2\u003E\\n\u003Cp\u003EThe HTRC Feature Reader library has three main objects: \u003Cstrong\u003EFeatureReader\u003C\u002Fstrong\u003E, \u003Cstrong\u003EVolume\u003C\u002Fstrong\u003E, and \u003Cstrong\u003EPage\u003C\u002Fstrong\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe \u003Cstrong\u003EFeatureReader\u003C\u002Fstrong\u003E object is the interface for loading the dataset files and making sense of them. The files are originally formatted in a notation called JSON (which \u003Cem\u003EProgramming Historian\u003C\u002Fem\u003E discusses \u003Ca href=\\\"\u002Flessons\u002Fjson-and-jq\\\"\u003Ehere\u003C\u002Fa\u003E) and compressed, which FeatureReader makes sense of and returns as Volume objects. A \u003Cstrong\u003EVolume\u003C\u002Fstrong\u003E is a representation of a single book or other work. This is where you access features about a work. Many features for a volume are collected from individual pages; to access Page information, you can use the \u003Cstrong\u003EPage\u003C\u002Fstrong\u003E object.\u003C\u002Fp\u003E\\n\u003Cp\u003ELet&#39;s load two volumes to understand how the FeatureReader works. Create a cell in the already-open Jupyter notebook and run the following code. This should give you the input shown below.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Efrom htrc_features import FeatureReader\\nimport os\\npaths = [os.path.join(&#39;data&#39;, &#39;sample-file1.json.bz2&#39;), os.path.join(&#39;data&#39;, &#39;sample-file2.json.bz2&#39;)]\\nfr = FeatureReader(paths)\\nfor vol in fr.volumes():\\n    print(vol.title)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cpre\u003E\u003Ccode\u003EJune \u002F by Edith Barnard Delano ; with illustrations.\\nYou never know your luck; being the story of a matrimonial deserter, by Gilbert Parker ... illustrated by W.L. Jacobs.\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EHere, the FeatureReader is imported and initialized with file paths pointing to two Extracted Features files. The files are in a directory called &#39;data&#39;. Different systems do file paths differently: Windows uses back slashes (&#39;data\\\\...&#39;) while Linux and Mac OS use forward slashes (&#39;data\u002F...&#39;). \u003Ccode\u003Eos.path.join\u003C\u002Fcode\u003E is used to make sure that the file path is correctly structured, a convention to ensure that code works on these different platforms.\u003C\u002Fp\u003E\\n\u003Cp\u003EWith \u003Ccode\u003Efr = FeatureReader(paths)\u003C\u002Fcode\u003E, the FeatureReader is initialized, meaning it is ready to use. An initialized FeatureReader is holding references to the file paths that we gave it, and will load them into Volume objects when asked.\u003C\u002Fp\u003E\\n\u003Cp\u003EConsider the last bit of code:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Efor vol in fr.volumes():\\n    print(vol.title)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThis code asks for volumes in a way that can be iterated through. The \u003Ccode\u003Efor\u003C\u002Fcode\u003E loop is saying to \u003Ccode\u003Efr.volumes()\u003C\u002Fcode\u003E, &quot;give me every single volume that you have, one by one.&quot; Each time the \u003Ccode\u003Efor\u003C\u002Fcode\u003E loop gets a volume, it starts calling it \u003Ccode\u003Evol\u003C\u002Fcode\u003E, runs what is inside the loop on it, then asks for the next one. In this case, we just told it to print the title of the volume.\u003C\u002Fp\u003E\\n\u003Cp\u003EYou may recognize \u003Ccode\u003Efor\u003C\u002Fcode\u003E loops from past experience iterating through what is known as a \u003Ccode\u003Elist\u003C\u002Fcode\u003E in Python. However, it is important to note that \u003Ccode\u003Efr.volumes()\u003C\u002Fcode\u003E is \u003Cem\u003Enot\u003C\u002Fem\u003E a list. If you try to access it directly, it won&#39;t print all the volumes; rather, it identifies itself as something known as a generator:\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;generator.png&quot; caption=&quot;Identifying a generator&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EWhat is a generator, and why do we iterate over it?\u003C\u002Fp\u003E\\n\u003Cp\u003EGenerators are the key to working with lots of data. They allow you to iterate over a set of items that don&#39;t exist yet, preparing them only when it is their turn to be acted upon.\u003C\u002Fp\u003E\\n\u003Cp\u003ERemember that there are 13.6 million volumes in the Extracted Features dataset. When coding at that scale, you need to be be mindful of two rules:\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003EDon&#39;t hold everything in memory: you can&#39;t. Use it, reduce it, and move on.\u003C\u002Fli\u003E\\n\u003Cli\u003EDon&#39;t devote cycles to processing something before you need it.\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Cp\u003EA generator simplifies such on-demand, short term usage. Think of it like a pizza shop making pizzas when a customer orders, versus one that prepares them beforehand. The traditional approach to iterating through data is akin to making \u003Cem\u003Eall\u003C\u002Fem\u003E the pizzas for the day before opening. Doing so would make the buying process quicker, but also adds a huge upfront time cost, needs larger ovens, and necessitates the space to hold all the pizzas at once. An alternate approach is to make pizzas on-demand when customers buy them, allowing the pizza place to work with smaller capacities and without having pizzas laying around the shop. This is the type of approach that a generator allows.\u003C\u002Fp\u003E\\n\u003Cp\u003EVolumes need to be prepared before you do anything with them, being read, decompressed and parsed. This &#39;initialization&#39; of a volume is done when you ask for the volume, \u003Cem\u003Enot\u003C\u002Fem\u003E when you create the FeatureReader. In the above code, after you run \u003Ccode\u003Efr = FeatureReader(paths)\u003C\u002Fcode\u003E, there are are still no \u003Ccode\u003EVolume\u003C\u002Fcode\u003E objects held behind the scenes: just the references to the file locations. The files are only read when their time comes in the loop on the generator \u003Ccode\u003Efr.volumes()\u003C\u002Fcode\u003E. Note that because of this one-by-one reading, the items of a generator cannot be accessed out of order (e.g. you cannot ask for the third item of \u003Ccode\u003Efr.volumes()\u003C\u002Fcode\u003E without going through the first two first).\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"whats-in-a-volume\\\"\u003EWhat&#39;s in a Volume?\u003C\u002Fh2\u003E\\n\u003Cp\u003ELet&#39;s take a closer look at what features are accessible for a Volume object. For clarity, we&#39;ll grab the first Volume to focus on, which can conveniently be accessed with the \u003Ccode\u003Efirst()\u003C\u002Fcode\u003E method. Any code you write can easily be run later with a \u003Ccode\u003Efor vol in fr.volumes()\u003C\u002Fcode\u003E loop.\u003C\u002Fp\u003E\\n\u003Cp\u003EAgain here, start a new code cell in the same notebook that you had open before and run the following code. The FeatureReader does not need to be loaded again: it is still initialized and accessible as \u003Ccode\u003Efr\u003C\u002Fcode\u003E from earlier.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E# Reading a single volume\\nvol = fr.first()\\nvol\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cpre\u003E\u003Ccode\u003E&lt;htrc_features.feature_reader.Volume at 0x1cf355a60f0&gt;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EWhile the majority of the HTRC Extracted Features dataset is \u003Cem\u003Efeatures\u003C\u002Fem\u003E, quantitative abstractions of a book&#39;s written content, there is also a small amount of metadata included for each volume. We already saw \u003Ccode\u003EVolume.title\u003C\u002Fcode\u003E accessed earlier. Other metadata attributes include:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003E\u003Ccode\u003EVolume.id\u003C\u002Fcode\u003E: A unique identifier for the volume in the HathiTrust and the HathiTrust Research Center.\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003EVolume.year\u003C\u002Fcode\u003E: The publishing date of the volume.\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003EVolume.language\u003C\u002Fcode\u003E: The classified language of the volume.\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003EVolume.oclc\u003C\u002Fcode\u003E: The OCLC control number(s).\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003EThe volume id can be used to pull more information from other sources. The scanned copy of the book can be found from the HathiTrust Digital Library, when available, by accessing \u003Ccode\u003Ehttp:\u002F\u002Fhdl.handle.net\u002F2027\u002F{VOLUME ID}\u003C\u002Fcode\u003E. In the feature reader, this url is retrieved by calling \u003Ccode\u003Evol.handle_url\u003C\u002Fcode\u003E:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Eprint(vol.handle_url)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cpre\u003E\u003Ccode\u003Ehttp:\u002F\u002Fhdl.handle.net\u002F2027\u002Fnyp.33433075749246\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;June-cover.PNG&quot; caption=&quot;Digital copy of sample book&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EHopefully by now you are growing more comfortable with the process of running code in a Jupyter notebook, starting a cell, writing code, and running the cell. A valuable property of this type of interactive coding is that there is room for error. An error doesn&#39;t cause the whole program to crash, requiring you to rerun everything from the start. Instead, just fix the code in your cell and try again.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn Jupyter, pressing the &#39;TAB&#39; key will guess at what you want to type next. Typing \u003Ccode\u003Evo\u003C\u002Fcode\u003E then TAB will fill in \u003Ccode\u003Evol\u003C\u002Fcode\u003E, typing \u003Ccode\u003EFea\u003C\u002Fcode\u003E then TAB will fill in \u003Ccode\u003EFeatureReader\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EAuto-completion with the tab key also provides more information about what you can get from an object. Try typing \u003Ccode\u003Evol.\u003C\u002Fcode\u003E (with the period) in a new cell, then press TAB. Jupyter shows everything that you can access for that Volume.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;autocomplete.png&quot; caption=&quot;Tab Autocomplete in Jupyter&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EThe Extracted Features dataset does not hold all the metadata that the HathiTrust has for the book. More in-depth metadata like genre and subject class needs to be grabbed from other sources, such as the \u003Ca href=\\\"https:\u002F\u002Fwww.hathitrust.org\u002Fbib_api\\\"\u003EHathiTrust Bibliographic API\u003C\u002Fa\u003E. The URL to access this information can be retrieved with \u003Ccode\u003Evol.ht_bib_url\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"our-first-feature-access-visualizing-words-per-page\\\"\u003EOur First Feature Access: Visualizing Words Per Page\u003C\u002Fh2\u003E\\n\u003Cp\u003EIt&#39;s time to access the first features of \u003Ccode\u003Evol\u003C\u002Fcode\u003E: a table of total words for every single page. These can be accessed by calling \u003Ccode\u003Evol.tokens_per_page()\u003C\u002Fcode\u003E. Try the following code.\u003C\u002Fp\u003E\\n\u003Cblockquote\u003E\\n\u003Cp\u003EIf you are using a Jupyter notebook, returning this table at the end of a cell formats it nicely in the browser. Below, you&#39;ll see us append \u003Ccode\u003E.head()\u003C\u002Fcode\u003E to the \u003Ccode\u003Etokens\u003C\u002Fcode\u003E table, which allows us to look at just the top few rows: the &#39;head&#39; of the data.\u003C\u002Fp\u003E\\n\u003C\u002Fblockquote\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Etokens = vol.tokens_per_page()\\n# Show just the first few rows, so we can look at what it looks like\\ntokens.head()\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cdiv\u003E\\n\u003Ctable border=\\\"1\\\" class=\\\"dataframe\\\"\u003E\\n  \u003Cthead\u003E\\n    \u003Ctr style=\\\"text-align: right;\\\"\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n      \u003Cth\u003Ecount\u003C\u002Fth\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Epage\u003C\u002Fth\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n    \u003C\u002Ftr\u003E\\n  \u003C\u002Fthead\u003E\\n  \u003Ctbody\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003E1\u003C\u002Fth\u003E\\n      \u003Ctd\u003E5\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003E2\u003C\u002Fth\u003E\\n      \u003Ctd\u003E0\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003E3\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003E4\u003C\u002Fth\u003E\\n      \u003Ctd\u003E0\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003E5\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n  \u003C\u002Ftbody\u003E\\n\u003C\u002Ftable\u003E\\n\u003C\u002Fdiv\u003E\\n\\n\\n\\n\u003Cblockquote\u003E\\n\u003Cp\u003ENo print! We didn&#39;t call &#39;print()&#39; to make Jupyter show the table. Instead, it automatically guessed that you want to display the information from the last code line of the cell.\u003C\u002Fp\u003E\\n\u003C\u002Fblockquote\u003E\\n\u003Cp\u003EThis is a straightforward table of information, similar to what you would see in Excel or Google Spreadsheets. Listed in the table are page numbers and the count of words on each page. With only two dimensions, it is trivial to plot the number of words per page. The table structure holding the data has a \u003Ccode\u003Eplot\u003C\u002Fcode\u003E method for data graphics. Without extra arguments, \u003Ccode\u003Etokens.plot()\u003C\u002Fcode\u003E will assume that you want a line chart with the page on the x-axis and word count on the y-axis.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E%matplotlib inline\\ntokens.plot()\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;draft_23_1.png&quot; caption=&quot;Output graph.&quot; %}\u003C\u002Fp\u003E\\n\u003Cblockquote\u003E\\n\u003Cp\u003E\u003Ccode\u003E%matplotlib inline\u003C\u002Fcode\u003E tells Jupyter to show the plotted image directly in the notebook web page. It only needs to be called once, and isn&#39;t needed if you&#39;re not using notebooks.\u003C\u002Fp\u003E\\n\u003C\u002Fblockquote\u003E\\n\u003Cp\u003EOn some systems, this may take some time the first time. It is clear that pages at the start of a book have fewer words per page, after which the count is fairly steady except for occasional valleys.\u003C\u002Fp\u003E\\n\u003Cp\u003EYou may have some guesses for what these patterns mean. A look at the \u003Ca href=\\\"http:\u002F\u002Fhdl.handle.net\u002F2027\u002Fnyp.33433074811310\\\"\u003Escans\u003C\u002Fa\u003E confirms that the large valleys are often illustration pages or blank pages, small valleys are chapter headings, and the upward pattern at the start is from front matter.\u003C\u002Fp\u003E\\n\u003Cp\u003ENot all books will have the same patterns so we can&#39;t just codify these correlations for millions of books. However, looking at this plot makes clear an inportant assumption in text and data mining: that there are patterns underlying even the basic statistics derived from a text. The trick is to identify the consistent and interesting patterns and teach them to a computer.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"understanding-dataframes\\\"\u003EUnderstanding DataFrames\u003C\u002Fh3\u003E\\n\u003Cp\u003EWait... how did we get here so quickly!? We went from a volume to a data visualization in two lines of code. The magic is in the data structure used to hold our table of data: a DataFrame.\u003C\u002Fp\u003E\\n\u003Cp\u003EA \u003Cstrong\u003EDataFrame\u003C\u002Fstrong\u003E is a type of object provided by the data analysis library, Pandas. \u003Cstrong\u003EPandas\u003C\u002Fstrong\u003E is very common for data analysis, allowing conveniences in Python that are found in statistical languages like R or Matlab.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn the first line, \u003Ccode\u003Evol.tokens_per_page()\u003C\u002Fcode\u003E returns a DataFrame, something that can be confirmed if you ask Python about its type with \u003Ccode\u003Etype(tokens)\u003C\u002Fcode\u003E. This means that \u003Cem\u003Eafter setting \u003Ccode\u003Etokens\u003C\u002Fcode\u003E, we&#39;re no longer working with HTRC-specific code, just book data held in a common and very robust table-like construct from Pandas\u003C\u002Fem\u003E. \u003Ccode\u003Etokens.head()\u003C\u002Fcode\u003E used a DataFrame method to look at the first few rows of the dataset, and \u003Ccode\u003Etokens.plot()\u003C\u002Fcode\u003E uses a method from Pandas to visualize data.\u003C\u002Fp\u003E\\n\u003Cp\u003EMany of the methods in the HTRC Feature Reader return DataFrames. The aim is to fit into the workflow of an experienced user, rather than requiring them to learn proprietary new formats. For new Python data mining users, learning to use the HTRC Feature Reader means learning many data mining skills that will translate to other uses.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"loading-a-token-list\\\"\u003ELoading a Token List\u003C\u002Fh2\u003E\\n\u003Cp\u003EThe information contained in \u003Ccode\u003Evol.tokens_per_page()\u003C\u002Fcode\u003E is minimal, a sum of all words in the body of each page.\\nThe Extracted Features dataset also provides token counts with much more granularity: for every part of speech (e.g. noun, verb) of every occurring capitalization of every word of every section (i.e. header, footer, body) of every page of the volume.\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Ccode\u003Etokens_per_page()\u003C\u002Fcode\u003E only kept the &quot;for every page&quot; grouping; \u003Ccode\u003Evol.tokenlist()\u003C\u002Fcode\u003E can be called to return section-, part-of-speech-, and word-specific details:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Etl = vol.tokenlist()\\n# Let&#39;s look at some words deeper into the book:\\n# from 1000th to 1100th row, skipping by 15 [1000:1100:15]\\ntl[1000:1100:15]\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cdiv\u003E\\n\u003Ctable border=\\\"1\\\" class=\\\"dataframe\\\"\u003E\\n  \u003Cthead\u003E\\n    \u003Ctr style=\\\"text-align: right;\\\"\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n      \u003Cth\u003Ecount\u003C\u002Fth\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Epage\u003C\u002Fth\u003E\\n      \u003Cth\u003Esection\u003C\u002Fth\u003E\\n      \u003Cth\u003Etoken\u003C\u002Fth\u003E\\n      \u003Cth\u003Epos\u003C\u002Fth\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n    \u003C\u002Ftr\u003E\\n  \u003C\u002Fthead\u003E\\n  \u003Ctbody\u003E\\n    \u003Ctr\u003E\\n      \u003Cth rowspan=\\\"2\\\" valign=\\\"top\\\"\u003E27\u003C\u002Fth\u003E\\n      \u003Cth rowspan=\\\"2\\\" valign=\\\"top\\\"\u003Ebody\u003C\u002Fth\u003E\\n      \u003Cth\u003Ethose\u003C\u002Fth\u003E\\n      \u003Cth\u003EDT\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Ewithin\u003C\u002Fth\u003E\\n      \u003Cth\u003EIN\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth rowspan=\\\"5\\\" valign=\\\"top\\\"\u003E28\u003C\u002Fth\u003E\\n      \u003Cth rowspan=\\\"5\\\" valign=\\\"top\\\"\u003Ebody\u003C\u002Fth\u003E\\n      \u003Cth\u003Ea\u003C\u002Fth\u003E\\n      \u003Cth\u003EDT\u003C\u002Fth\u003E\\n      \u003Ctd\u003E3\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Ebe\u003C\u002Fth\u003E\\n      \u003Cth\u003EVB\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Edeserted\u003C\u002Fth\u003E\\n      \u003Cth\u003EVBN\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Efaintly\u003C\u002Fth\u003E\\n      \u003Cth\u003ERB\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Eimportant\u003C\u002Fth\u003E\\n      \u003Cth\u003EJJ\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n  \u003C\u002Ftbody\u003E\\n\u003C\u002Ftable\u003E\\n\u003C\u002Fdiv\u003E\\n\\n\\n\\n\u003Cp\u003EAs before, the data is returned as a Pandas DataFrame. This time, there is much more information. Consider a single row:\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;single-row-tokencount.png&quot; caption=&quot;Single row of tokenlist.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EThe columns in bold are an index. Unlike the typical one-dimensional index seen before, here there are four dimensions to the index: page, section, token, and pos. This row says that for the 24th page, in the body section (i.e. ignoring any words in the header or footer), the word &#39;years&#39; occurs 1 time as an plural noun. The part-of-speech tag for a plural noun, \u003Ccode\u003ENNS\u003C\u002Fcode\u003E, follows the \u003Ca href=\\\"https:\u002F\u002Fwww.ling.upenn.edu\u002Fcourses\u002FFall_2003\u002Fling001\u002Fpenn_treebank_pos.html\\\"\u003EPenn Treebank\u003C\u002Fa\u003E definition.\u003C\u002Fp\u003E\\n\u003Cblockquote\u003E\\n\u003Cp\u003EThe &quot;words&quot; on the first page seems to be OCR errors for the cover of the book. The HTRC Feature Reader refers to &quot;pages&quot; as the $$n^{th}$$ scanned image of the volume, not the actual number printed on the page. This is why &quot;page 1&quot; for this example is the cover.\u003C\u002Fp\u003E\\n\u003C\u002Fblockquote\u003E\\n\u003Cp\u003ETokenlists can be retrieved with arguments that combine information by certain dimensions, such as \u003Ccode\u003Ecase\u003C\u002Fcode\u003E, \u003Ccode\u003Epos\u003C\u002Fcode\u003E, or \u003Ccode\u003Epage\u003C\u002Fcode\u003E. For example, \u003Ccode\u003Ecase=False\u003C\u002Fcode\u003E specified that &quot;Jaguar&quot; and &quot;jaguar&quot; should be counted together. You may also notice that, by default, only &#39;body&#39; is returned, a default that can be overridden.\u003C\u002Fp\u003E\\n\u003Cp\u003ELook at the following list of commands: can you guess what the output will look like? Try for yourself and observe how the output changes.\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003E\u003Ccode\u003Evol.tokenlist(case=False)\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003Evol.tokenlist(pos=False)\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003Evol.tokenlist(pages=False, case=False, pos=False)\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003Evol.tokenlist(section=&#39;header&#39;)\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003Evol.tokenlist(section=&#39;group&#39;)\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003EDetails for these arguments are available in the code \u003Ca href=\\\"http:\u002F\u002Fhtrc.github.io\u002Fhtrc-feature-reader\u002Fhtrc_features\u002Ffeature_reader.m.html#htrc_features.feature_reader.Volume.tokenlist\\\"\u003Edocumentation\u003C\u002Fa\u003E for the Feature Reader.\u003C\u002Fp\u003E\\n\u003Cp\u003EJupyter provides another convenience here. Documentation can be accessed within the notebook by adding a &#39;?&#39; to the start of a piece of code. Try it with \u003Ccode\u003E?vol.tokenlist\u003C\u002Fcode\u003E, or with other objects or variables.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"working-with-dataframes\\\"\u003EWorking with DataFrames\u003C\u002Fh2\u003E\\n\u003Cp\u003EThe Pandas DataFrame type returned by the HTRC Feature Reader is very malleable. To work with the tokenlist that you retrieved earlier, three skills are particularily valuable:\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003ESelecting subsets by a condition\u003C\u002Fli\u003E\\n\u003Cli\u003ESlicing by named row index\u003C\u002Fli\u003E\\n\u003Cli\u003EGrouping and aggregating\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Ch3 id=\\\"selecting-subsets-of-a-dataframe-by-a-condition\\\"\u003ESelecting Subsets of a DataFrame by a Condition\u003C\u002Fh3\u003E\\n\u003Cp\u003EConsider this example: \u003Cem\u003EI only want to look at tokens that occur more than a hundred times in the book.\u003C\u002Fem\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003ERemembering that the table-like output from the HTRC Feature Reader is a Pandas DataFrame, the way to pursue this goal is to learn to filter and subset DataFrames. Knowing how to do so is important for working with just the data that you need.\u003C\u002Fp\u003E\\n\u003Cp\u003ETo subset individual rows of a DataFrame, you can provide a series of True\u002FFalse values to the DataFrame, formatted in square brackets. When True, the DataFrame returns that row; when False, the row is excluded from what is returned.\u003C\u002Fp\u003E\\n\u003Cp\u003ETo see this in context, first load a basic tokenlist without parts-of-speech or individual pages:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Etl_simple = vol.tokenlist(pos=False, pages=False)\\n# .sample(5) returns five random words from the full result\\ntl_simple.sample(5)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cdiv\u003E\\n\u003Ctable border=\\\"1\\\" class=\\\"dataframe\\\"\u003E\\n  \u003Cthead\u003E\\n    \u003Ctr style=\\\"text-align: right;\\\"\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n      \u003Cth\u003Ecount\u003C\u002Fth\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Esection\u003C\u002Fth\u003E\\n      \u003Cth\u003Etoken\u003C\u002Fth\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n    \u003C\u002Ftr\u003E\\n  \u003C\u002Fthead\u003E\\n  \u003Ctbody\u003E\\n    \u003Ctr\u003E\\n      \u003Cth rowspan=\\\"5\\\" valign=\\\"top\\\"\u003Ebody\u003C\u002Fth\u003E\\n      \u003Cth\u003Ehalleluya\u003C\u002Fth\u003E\\n      \u003Ctd\u003E4\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Erealty\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Eaddressed\u003C\u002Fth\u003E\\n      \u003Ctd\u003E4\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Ewin-\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Ebroke\u003C\u002Fth\u003E\\n      \u003Ctd\u003E3\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n  \u003C\u002Ftbody\u003E\\n\u003C\u002Ftable\u003E\\n\u003C\u002Fdiv\u003E\\n\\n\\n\\n\u003Cp\u003ETo select just the relevant tokens, we need to look at each row and evaluate whether it matches the criteria that &quot;this token has a count greater than 100&quot;. Let&#39;s try to convert that requirement to code.\u003C\u002Fp\u003E\\n\u003Cp\u003E&quot;This token has a count&quot; means that we are concerned specifically with the &#39;count&#39; column, which can be singled out from the \u003Ccode\u003Etl\u003C\u002Fcode\u003E table with \u003Ccode\u003Etl[&#39;count&#39;]\u003C\u002Fcode\u003E. &quot;greater than 100&quot; is formalized as \u003Ccode\u003E&gt; 100\u003C\u002Fcode\u003E. Putting it together, try the following and see what you get:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Etl_simple[&#39;count&#39;] &gt; 100\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EIt is a DataFrame of True\u002FFalse values. Each value indicates whether the &#39;count&#39; column in the corresponding row matches the criteria or not. We haven&#39;t selected a subset yet, we simply asked a question and were told for each row when the question was true or false.\u003C\u002Fp\u003E\\n\u003Cblockquote\u003E\\n\u003Cp\u003EYou may wonder why section and token are still seen, even though &#39;count&#39; was selected. These are part of the DataFrame \u003Cstrong\u003Eindex\u003C\u002Fstrong\u003E, so they&#39;re considered part of the information \u003Cem\u003Eabout\u003C\u002Fem\u003E that row rather than data \u003Cem\u003Ein\u003C\u002Fem\u003E the row. You can convert the index to data columns with \u003Ccode\u003Ereset_index()\u003C\u002Fcode\u003E. In this lesson we will keep the index intact, though there are advanced cases where there are benefits to resetting it.\u003C\u002Fp\u003E\\n\u003C\u002Fblockquote\u003E\\n\u003Cp\u003EArmed with the True\u002FFalse values of whether each token&#39;s &#39;count&#39; value is or isn&#39;t greater than 100, we can give those values to \u003Ccode\u003Etl_simple\u003C\u002Fcode\u003E in square brackets.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Ematches = tl_simple[&#39;count&#39;] &gt; 100\\ntl_simple[matches].sample(5)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cdiv\u003E\\n\u003Ctable border=\\\"1\\\" class=\\\"dataframe\\\"\u003E\\n  \u003Cthead\u003E\\n    \u003Ctr style=\\\"text-align: right;\\\"\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n      \u003Cth\u003Ecount\u003C\u002Fth\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Esection\u003C\u002Fth\u003E\\n      \u003Cth\u003Etoken\u003C\u002Fth\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n    \u003C\u002Ftr\u003E\\n  \u003C\u002Fthead\u003E\\n  \u003Ctbody\u003E\\n    \u003Ctr\u003E\\n      \u003Cth rowspan=\\\"5\\\" valign=\\\"top\\\"\u003Ebody\u003C\u002Fth\u003E\\n      \u003Cth\u003Ethey\u003C\u002Fth\u003E\\n      \u003Ctd\u003E127\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Ehave\u003C\u002Fth\u003E\\n      \u003Ctd\u003E210\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003EThe\u003C\u002Fth\u003E\\n      \u003Ctd\u003E107\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Eabout\u003C\u002Fth\u003E\\n      \u003Ctd\u003E110\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003E,\u003C\u002Fth\u003E\\n      \u003Ctd\u003E3258\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n  \u003C\u002Ftbody\u003E\\n\u003C\u002Ftable\u003E\\n\u003C\u002Fdiv\u003E\\n\\n\\n\\n\u003Cp\u003EYou can move the comparison straight into the square brackets, the more conventional equivalent of the above:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Etl_simple[tl_simple[&#39;count&#39;] &gt; 100].sample(5)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cdiv\u003E\\n\u003Ctable border=\\\"1\\\" class=\\\"dataframe\\\"\u003E\\n  \u003Cthead\u003E\\n    \u003Ctr style=\\\"text-align: right;\\\"\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n      \u003Cth\u003Ecount\u003C\u002Fth\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Esection\u003C\u002Fth\u003E\\n      \u003Cth\u003Etoken\u003C\u002Fth\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n    \u003C\u002Ftr\u003E\\n  \u003C\u002Fthead\u003E\\n  \u003Ctbody\u003E\\n    \u003Ctr\u003E\\n      \u003Cth rowspan=\\\"5\\\" valign=\\\"top\\\"\u003Ebody\u003C\u002Fth\u003E\\n      \u003Cth\u003Enot\u003C\u002Fth\u003E\\n      \u003Ctd\u003E220\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Ehad\u003C\u002Fth\u003E\\n      \u003Ctd\u003E455\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Ehave\u003C\u002Fth\u003E\\n      \u003Ctd\u003E210\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003E,\u003C\u002Fth\u003E\\n      \u003Ctd\u003E3258\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Ehis\u003C\u002Fth\u003E\\n      \u003Ctd\u003E206\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n  \u003C\u002Ftbody\u003E\\n\u003C\u002Ftable\u003E\\n\u003C\u002Fdiv\u003E\\n\\n\\n\\n\u003Cp\u003EAs might be expected, many of the tokens that occur very often are common words like &quot;she&quot; and &quot;and&quot;, as well as various punctuation.\u003C\u002Fp\u003E\\n\u003Cp\u003EMultiple conditions can be chained with \u003Ccode\u003E&amp;\u003C\u002Fcode\u003E (and) or \u003Ccode\u003E|\u003C\u002Fcode\u003E (or), using regular brackets so that Python knows the order of operations. For example, words with a count greater than 150 \u003Cem\u003Eand\u003C\u002Fem\u003E a count less than 200 are selected in this way:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Etl_simple[(tl_simple[&#39;count&#39;] &gt; 150) &amp; (tl_simple[&#39;count&#39;] &lt; 200)]\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cdiv\u003E\\n\u003Ctable border=\\\"1\\\" class=\\\"dataframe\\\"\u003E\\n  \u003Cthead\u003E\\n    \u003Ctr style=\\\"text-align: right;\\\"\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n      \u003Cth\u003Ecount\u003C\u002Fth\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Esection\u003C\u002Fth\u003E\\n      \u003Cth\u003Etoken\u003C\u002Fth\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n    \u003C\u002Ftr\u003E\\n  \u003C\u002Fthead\u003E\\n  \u003Ctbody\u003E\\n    \u003Ctr\u003E\\n      \u003Cth rowspan=\\\"7\\\" valign=\\\"top\\\"\u003Ebody\u003C\u002Fth\u003E\\n      \u003Cth\u003EMr.\u003C\u002Fth\u003E\\n      \u003Ctd\u003E159\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Ebe\u003C\u002Fth\u003E\\n      \u003Ctd\u003E196\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Ebut\u003C\u002Fth\u003E\\n      \u003Ctd\u003E178\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Edo\u003C\u002Fth\u003E\\n      \u003Ctd\u003E179\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Eis\u003C\u002Fth\u003E\\n      \u003Ctd\u003E170\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Eon\u003C\u002Fth\u003E\\n      \u003Ctd\u003E190\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Eâ€”\u003C\u002Fth\u003E\\n      \u003Ctd\u003E167\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n  \u003C\u002Ftbody\u003E\\n\u003C\u002Ftable\u003E\\n\u003C\u002Fdiv\u003E\\n\\n\\n\\n\u003Ch3 id=\\\"slicing-dataframes\\\"\u003ESlicing DataFrames\u003C\u002Fh3\u003E\\n\u003Cp\u003EAbove, subsets of the DataFrame were selected based on a matching criteria for columns. It is also possible to select a DataFrame subset by specifying the values of its index, a process called \u003Cstrong\u003Eslicing\u003C\u002Fstrong\u003E. For example, you can ask, \u003Cem\u003E&quot;give me all the verbs for pages 9-12&quot;\u003C\u002Fem\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn the DataFrame returned by \u003Ccode\u003Evol.tokenlist()\u003C\u002Fcode\u003E, page, section, token, and POS were part of the index (try the command \u003Ccode\u003Etl.index.names\u003C\u002Fcode\u003E to confirm). One can think of an index as the margin content of an Excel spreadsheet: the letters along the top and numbers along the left side are the indices. A cell can be referred to as A1, A2, B1... In Pandas, however, you can name these, so instead of A, B, C, or 1,2,3, columns and rows can be referred to by more descriptive names. You can also have multiple levels, so you&#39;re not bound by the two-dimensions of a table format. With a multi-indexed DataFrame, you can ask for \u003Ccode\u003EPage=24,section=Body, ...\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;Excel.PNG&quot; caption=&quot;One can think of an index as the margin notations in Excel (i.e. 1,2,3... and A,B,C,..), except it can be named and can have multiple levels.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003ESlicing a DataFrame against a labelled index is done using \u003Ccode\u003EDataFrame.loc[]\u003C\u002Fcode\u003E. Try the following examples and see what is returned:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003ESelect information from page 17:\u003Cul\u003E\\n\u003Cli\u003E\u003Ccode\u003Etl.loc[(17),]\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003ESelect &#39;body&#39; section of page 17:\u003Cul\u003E\\n\u003Cli\u003E\u003Ccode\u003Etl.loc[(17, &#39;body&#39;),]\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003ESelect counts of the word &#39;Anne&#39; in the &#39;body&#39; section of page 17:\u003Cul\u003E\\n\u003Cli\u003E\u003Ccode\u003Etl.loc[(17, &#39;body&#39;, &#39;Anne&#39;),]\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003EThe levels of the index are specified in order, so in this case the first value refers to &#39;page&#39;, then &#39;section&#39;, and so on. To skip specifying anything for an index level -- that is, to select everything for that level -- \u003Ccode\u003Eslice(None)\u003C\u002Fcode\u003E can be used as a placeholder:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003ESelect counts of the word &#39;Anne&#39; for all pages and all page sections\u003Cul\u003E\\n\u003Cli\u003E\u003Ccode\u003Etl.loc[(slice(None), slice(None), &quot;Anne&quot;),]\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003EFinally, it is possible to select multiple labels for a level of the index, with a list of labels (i.e. \u003Ccode\u003E[&#39;label1&#39;, &#39;label2&#39;]\u003C\u002Fcode\u003E) or a sequence covering everything from one value to another (i.e. \u003Ccode\u003Eslice(start, end)\u003C\u002Fcode\u003E):\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003ESelect pages 37, 38, and 52\u003Cul\u003E\\n\u003Cli\u003E\u003Ccode\u003Etl.loc[([37, 38, 52]),]\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003ESelect all pages from 37 to 40\u003Cul\u003E\\n\u003Cli\u003E\u003Ccode\u003Etl.loc[(slice(37, 40)),]\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003ESelect counts for &#39;Anne&#39; or &#39;Hilary&#39; from all pages\u003Cul\u003E\\n\u003Cli\u003E\u003Ccode\u003Etl.loc[(slice(None), slice(None), [&quot;Anne&quot;, &quot;Hilary&quot;]),]\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cblockquote\u003E\\n\u003Cp\u003EThe reason for the comma in \u003Ccode\u003Etl.loc[(...),]\u003C\u002Fcode\u003E is because columns can be selected in the same way after the comma. Pandas DataFrames can have a multiple-level index for columns, but the HTRC Feature Reader does not use this.\u003C\u002Fp\u003E\\n\u003C\u002Fblockquote\u003E\\n\u003Cp\u003EKnowing how to slice, let&#39;s try to find the word &quot;CHAPTER&quot; in this book, and compare where that shows up to the token-per-page pattern previously plotted.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe token list we previously set to \u003Ccode\u003Etl\u003C\u002Fcode\u003E only included body text; to include headers and footers in a search for \u003Ccode\u003ECHAPTER\u003C\u002Fcode\u003E we&#39;ll grab a new tokenlist with \u003Ccode\u003Esection=&#39;all&#39;\u003C\u002Fcode\u003E specified.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Etl_all = vol.tokenlist(section=&#39;all&#39;)\\nchapter_pages = tl_all.loc[(slice(None), slice(None), &quot;CHAPTER&quot;),]\\nchapter_pages\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cdiv\u003E\\n\u003Ctable border=\\\"1\\\" class=\\\"dataframe\\\"\u003E\\n  \u003Cthead\u003E\\n    \u003Ctr style=\\\"text-align: right;\\\"\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n      \u003Cth\u003Ecount\u003C\u002Fth\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Epage\u003C\u002Fth\u003E\\n      \u003Cth\u003Esection\u003C\u002Fth\u003E\\n      \u003Cth\u003Etoken\u003C\u002Fth\u003E\\n      \u003Cth\u003Epos\u003C\u002Fth\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n    \u003C\u002Ftr\u003E\\n  \u003C\u002Fthead\u003E\\n  \u003Ctbody\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003E19\u003C\u002Fth\u003E\\n      \u003Cth\u003Eheader\u003C\u002Fth\u003E\\n      \u003Cth\u003ECHAPTER\u003C\u002Fth\u003E\\n      \u003Cth\u003ENNP\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003E35\u003C\u002Fth\u003E\\n      \u003Cth\u003Eheader\u003C\u002Fth\u003E\\n      \u003Cth\u003ECHAPTER\u003C\u002Fth\u003E\\n      \u003Cth\u003ENNP\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003E56\u003C\u002Fth\u003E\\n      \u003Cth\u003Eheader\u003C\u002Fth\u003E\\n      \u003Cth\u003ECHAPTER\u003C\u002Fth\u003E\\n      \u003Cth\u003ENNP\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003E73\u003C\u002Fth\u003E\\n      \u003Cth\u003Eheader\u003C\u002Fth\u003E\\n      \u003Cth\u003ECHAPTER\u003C\u002Fth\u003E\\n      \u003Cth\u003ENNP\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003E91\u003C\u002Fth\u003E\\n      \u003Cth\u003Eheader\u003C\u002Fth\u003E\\n      \u003Cth\u003ECHAPTER\u003C\u002Fth\u003E\\n      \u003Cth\u003ENNP\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003E115\u003C\u002Fth\u003E\\n      \u003Cth\u003Eheader\u003C\u002Fth\u003E\\n      \u003Cth\u003ECHAPTER\u003C\u002Fth\u003E\\n      \u003Cth\u003ENNP\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003E141\u003C\u002Fth\u003E\\n      \u003Cth\u003Eheader\u003C\u002Fth\u003E\\n      \u003Cth\u003ECHAPTER\u003C\u002Fth\u003E\\n      \u003Cth\u003ENNP\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003E158\u003C\u002Fth\u003E\\n      \u003Cth\u003Eheader\u003C\u002Fth\u003E\\n      \u003Cth\u003ECHAPTER\u003C\u002Fth\u003E\\n      \u003Cth\u003ENNP\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003E174\u003C\u002Fth\u003E\\n      \u003Cth\u003Eheader\u003C\u002Fth\u003E\\n      \u003Cth\u003ECHAPTER\u003C\u002Fth\u003E\\n      \u003Cth\u003ENNP\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003E193\u003C\u002Fth\u003E\\n      \u003Cth\u003Eheader\u003C\u002Fth\u003E\\n      \u003Cth\u003ECHAPTER\u003C\u002Fth\u003E\\n      \u003Cth\u003ENNP\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003E217\u003C\u002Fth\u003E\\n      \u003Cth\u003Ebody\u003C\u002Fth\u003E\\n      \u003Cth\u003ECHAPTER\u003C\u002Fth\u003E\\n      \u003Cth\u003ENNP\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003E231\u003C\u002Fth\u003E\\n      \u003Cth\u003Eheader\u003C\u002Fth\u003E\\n      \u003Cth\u003ECHAPTER\u003C\u002Fth\u003E\\n      \u003Cth\u003ENNP\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003E246\u003C\u002Fth\u003E\\n      \u003Cth\u003Eheader\u003C\u002Fth\u003E\\n      \u003Cth\u003ECHAPTER\u003C\u002Fth\u003E\\n      \u003Cth\u003ENNP\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n  \u003C\u002Ftbody\u003E\\n\u003C\u002Ftable\u003E\\n\u003C\u002Fdiv\u003E\\n\\n\\n\\n\u003Cp\u003EEarlier, token counts were visualized using \u003Ccode\u003Etokens.plot()\u003C\u002Fcode\u003E, a built-in function of DataFrames that uses the Matplotlib visualization library.\u003C\u002Fp\u003E\\n\u003Cp\u003EWe can add to the earlier visualization by using Matplotlib directly. Try the following code in a new cell, which goes through every page number in the earlier search for &#39;CHAPTER&#39; and adds a red vertical line at the place in the chart with \u003Ccode\u003Ematplotlib.pyplot.axvline()\u003C\u002Fcode\u003E:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E# Get just the page numbers from the search for &quot;CHAPTER&quot;\\npage_numbers = chapter_pages.index.get_level_values(&#39;page&#39;)\\n\\n# Visualize the tokens-per-page from before\\ntokens.plot()\\n\\n# Add vertical lines for pages with &quot;CHAPTER&quot;\\nimport matplotlib.pyplot as plt\\nfor page_number in page_numbers:\\n    plt.axvline(x=page_number, color=&#39;red&#39;)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;draft_41_0.png&quot; caption=&quot;Output graph.&quot; %}\u003C\u002Fp\u003E\\n\u003Cblockquote\u003E\\n\u003Cp\u003EAdvanced: Though slicing with \u003Ccode\u003Eloc\u003C\u002Fcode\u003E is more common when working with the index, it is possible to create a True\u002FFalse list from an index to select rows as we did earlier. Here&#39;s an advanced example that grabs the &#39;token&#39; part of the index and, using the \u003Ccode\u003Eisalpha()\u003C\u002Fcode\u003E string method that Pandas provides, filters to fully alphabetical words.\u003C\u002Fp\u003E\\n\u003C\u002Fblockquote\u003E\\n\u003Cpre\u003E\u003Ccode\u003Etoken_idx = tl.index.get_level_values(&quot;token&quot;)\\ntl[token_idx.str.isalpha()]\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EReaders familiar with regular expressions (see \u003Ca href=\\\"\u002Flessons\u002Funderstanding-regular-expressions\\\"\u003EUnderstanding Regular Expressions\u003C\u002Fa\u003E by Doug Knox) can adapt this example for even more robust selection using the \u003Ccode\u003Econtains()\u003C\u002Fcode\u003E string method.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"sorting-dataframes\\\"\u003ESorting DataFrames\u003C\u002Fh2\u003E\\n\u003Cp\u003EA DataFrame can be sorted with \u003Ccode\u003EDataFrame.sort_values()\u003C\u002Fcode\u003E, specifying the column to sort by as the first argument. By default, sorting is done in ascending order:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Etl_simple.sort_values(&#39;count&#39;).head()\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cdiv\u003E\\n\u003Ctable border=\\\"1\\\" class=\\\"dataframe\\\"\u003E\\n  \u003Cthead\u003E\\n    \u003Ctr style=\\\"text-align: right;\\\"\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n      \u003Cth\u003Ecount\u003C\u002Fth\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Esection\u003C\u002Fth\u003E\\n      \u003Cth\u003Etoken\u003C\u002Fth\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n    \u003C\u002Ftr\u003E\\n  \u003C\u002Fthead\u003E\\n  \u003Ctbody\u003E\\n    \u003Ctr\u003E\\n      \u003Cth rowspan=\\\"5\\\" valign=\\\"top\\\"\u003Ebody\u003C\u002Fth\u003E\\n      \u003Cth\u003Egratified\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Ereminding\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Edome\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Eremembering\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Eremains\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n  \u003C\u002Ftbody\u003E\\n\u003C\u002Ftable\u003E\\n\u003C\u002Fdiv\u003E\\n\\n\\n\\n\u003Cp\u003EDescending order is possible with the argument \u003Ccode\u003Eascending=False\u003C\u002Fcode\u003E, which puts the most common tokens at the top. For example:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Etl_simple.sort_values(&#39;count&#39;, ascending=False).head()\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cdiv\u003E\\n\u003Ctable border=\\\"1\\\" class=\\\"dataframe\\\"\u003E\\n  \u003Cthead\u003E\\n    \u003Ctr style=\\\"text-align: right;\\\"\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n      \u003Cth\u003Ecount\u003C\u002Fth\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Esection\u003C\u002Fth\u003E\\n      \u003Cth\u003Etoken\u003C\u002Fth\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n    \u003C\u002Ftr\u003E\\n  \u003C\u002Fthead\u003E\\n  \u003Ctbody\u003E\\n    \u003Ctr\u003E\\n      \u003Cth rowspan=\\\"5\\\" valign=\\\"top\\\"\u003Ebody\u003C\u002Fth\u003E\\n      \u003Cth\u003E,\u003C\u002Fth\u003E\\n      \u003Ctd\u003E3258\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003E\\\"\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1670\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Ethe\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1565\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003E.\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1532\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Eand\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1252\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n  \u003C\u002Ftbody\u003E\\n\u003C\u002Ftable\u003E\\n\u003C\u002Fdiv\u003E\\n\\n\\n\\n\u003Cp\u003EThe most common tokens are &#39;the&#39; and &#39;and&#39;, alongside punctuation.\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Cem\u003EExercise: Try to retrieve the five most-common tokens used as a noun (&#39;NNP&#39;) or a plural noun (&#39;NNS&#39;) in the book\u003C\u002Fem\u003E. You will have to get a new tokenlist, without pages but with parts-of-speech, then slice by the criteria, sort, and output the first five rows. (\u003Ca href=\\\"https:\u002F\u002Fgist.github.com\u002Forganisciak\u002F163e59ea6cf71c3cd12de410d075567c\\\"\u003ESolution\u003C\u002Fa\u003E)\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"grouping-dataframes\\\"\u003EGrouping DataFrames\u003C\u002Fh3\u003E\\n\u003Cp\u003EUp to this point, the token count DataFrames have been subsetted, but not modified from the way they were returned by the HTRC Feature Reader. There are many cases where one may want to perform aggregation or transformation based on subsets of data. To do this, Pandas supports the &#39;split-apply-combine&#39; pattern (Wickham 2011).\u003C\u002Fp\u003E\\n\u003Cp\u003ESplit-apply-combine refers to the process of dividing a dataset into groups (\u003Cem\u003Esplit\u003C\u002Fem\u003E), performing some activity for each of those groups (\u003Cem\u003Eapply\u003C\u002Fem\u003E), and joining the new groups back together into a single DataFrame (\u003Cem\u003Ecombine\u003C\u002Fem\u003E).\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;split-apply-combine.png&quot; caption=&quot;Graph demonstrating Split-Apply-Combine.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;example-split-apply-combine.png&quot; caption=&quot;Example of Split-Apply-Combine, averaging movie grosses by director.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003ESplit-apply-combine processes are supported on DataFrames with \u003Ccode\u003Egroupby()\u003C\u002Fcode\u003E, which tells Pandas to split by some criteria. From there, it is possible to apply some change to each group individually, after which Pandas combines the affected groups into a single DataFrame again.\u003C\u002Fp\u003E\\n\u003Cp\u003ETry the following, can you tell what happens?\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Etl.groupby(level=[&quot;pos&quot;]).sum()\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe output is a count of how often each part-of-speech tag (&quot;pos&quot;) occurs in the entire book.\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003E\u003Cem\u003ESplit\u003C\u002Fem\u003E with \u003Ccode\u003Egroupby()\u003C\u002Fcode\u003E: We took the token count dataframe that is set to \u003Ccode\u003Etl\u003C\u002Fcode\u003E and grouped by the part-of-speech (\u003Ccode\u003Epos\u003C\u002Fcode\u003E) level of the index. This means that rather than thinking in terms of rows, Pandas is now thinking of the \u003Ccode\u003Etl\u003C\u002Fcode\u003E DataFrame as a series of smaller groups, the groups selected by a common value for part of speech. So, all the personal pronouns (&quot;PRP&quot;) are in one group, and all the adverbs (&quot;RB&quot;) are in another, and so on.\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cem\u003EApply\u003C\u002Fem\u003E with \u003Ccode\u003Esum()\u003C\u002Fcode\u003E: These groups were sent to an apply function, \u003Ccode\u003Esum()\u003C\u002Fcode\u003E. Sum is an aggregation function, so it sums all the information in the &#39;count&#39; column for each group. For example, all the rows of data in the adverb group are summed up into a single count of all adverbs.\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cem\u003ECombine\u003C\u002Fem\u003E: The combine step is implicit: the DataFrame knows from the \u003Ccode\u003Egroupby\u003C\u002Fcode\u003E pattern to take everything that the apply function gives back (in the case of &#39;sum&#39;, just one row for every group) and stick it together.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003E\u003Ccode\u003Esum()\u003C\u002Fcode\u003E is one of many convenient functions \u003Ca href=\\\"http:\u002F\u002Fpandas.pydata.org\u002Fpandas-docs\u002Fstable\u002Fgroupby.html\\\"\u003Ebuilt-in\u003C\u002Fa\u003E to Pandas. Other useful functions are \u003Ccode\u003Emean()\u003C\u002Fcode\u003E, \u003Ccode\u003Ecount()\u003C\u002Fcode\u003E, \u003Ccode\u003Emax()\u003C\u002Fcode\u003E. It is also possible to send your groups to any function that you write with \u003Ccode\u003Eapply()\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cblockquote\u003E\\n\u003Cp\u003Egroupby can be used on data columns or an index. To run against an index, use \u003Ccode\u003Elevel=[index_level_name]\u003C\u002Fcode\u003E as above. To group against columns, use \u003Ccode\u003Eby=[column_name]\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003C\u002Fblockquote\u003E\\n\u003Cp\u003EBelow are some examples of grouping token counts.\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003EFind most common tokens in the entire volume (sorting by most to least occurrences)\u003Cul\u003E\\n\u003Cli\u003E\u003Ccode\u003Etl.groupby(level=&quot;token&quot;).sum().sort_values(&quot;count&quot;, ascending=False)\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003ECount how many pages each token\u002Fpos combination occurs on\u003Cul\u003E\\n\u003Cli\u003E\u003Ccode\u003Etl.groupby(level=[&quot;token&quot;, &quot;pos&quot;]).count()\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003ERemember from earlier that certain information can be called by sending arguments to \u003Ccode\u003Evol.tokenlist()\u003C\u002Fcode\u003E, so you don&#39;t always have to do the grouping yourself.\u003C\u002Fp\u003E\\n\u003Cp\u003EWith \u003Ccode\u003Esum\u003C\u002Fcode\u003E, the data is being reduced: only one row is left for each group. It is also possible to &#39;transform&#39; a group, where the same number of rows are returned. This is useful if processing is necessary based on the group statistics, such as percentages. Here is an advanced example of transformation, a \u003Ca href=\\\"https:\u002F\u002Fweb.archive.org\u002Fweb\u002F20161108211721\u002Fhttps:\u002F\u002Fporganized.com\u002F2016\u002F03\u002F09\u002Fterm-weighting-for-humanists\u002F\\\"\u003ETF*IDF\u003C\u002Fa\u003E function. TF*IDF weighs a token&#39;s value to a document based on how common it is. In this case, it highlights words that are notable for a page but not the entire book.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Efrom numpy import log\\ndef tfidf(x):\\n    return x * log(1+vol.page_count \u002F x.count())\\n# Will take a few seconds to run, depending on your system\\nidf_scores = tl.groupby(level=[&quot;token&quot;]).transform(tfidf)\\nidf_scores[1000:1100:30]\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cdiv\u003E\\n\u003Ctable border=\\\"1\\\" class=\\\"dataframe\\\"\u003E\\n  \u003Cthead\u003E\\n    \u003Ctr style=\\\"text-align: right;\\\"\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n      \u003Cth\u003Ecount\u003C\u002Fth\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Epage\u003C\u002Fth\u003E\\n      \u003Cth\u003Esection\u003C\u002Fth\u003E\\n      \u003Cth\u003Etoken\u003C\u002Fth\u003E\\n      \u003Cth\u003Epos\u003C\u002Fth\u003E\\n      \u003Cth\u003E\u003C\u002Fth\u003E\\n    \u003C\u002Ftr\u003E\\n  \u003C\u002Fthead\u003E\\n  \u003Ctbody\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003E24\u003C\u002Fth\u003E\\n      \u003Cth\u003Ebody\u003C\u002Fth\u003E\\n      \u003Cth\u003Eyears\u003C\u002Fth\u003E\\n      \u003Cth\u003ENNS\u003C\u002Fth\u003E\\n      \u003Ctd\u003E2.315830\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth rowspan=\\\"3\\\" valign=\\\"top\\\"\u003E25\u003C\u002Fth\u003E\\n      \u003Cth rowspan=\\\"3\\\" valign=\\\"top\\\"\u003Ebody\u003C\u002Fth\u003E\\n      \u003Cth\u003Easked\u003C\u002Fth\u003E\\n      \u003Cth\u003EVBD\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1.730605\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003Ehim\u003C\u002Fth\u003E\\n      \u003Cth\u003EPRP\u003C\u002Fth\u003E\\n      \u003Ctd\u003E2.994040\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n    \u003Ctr\u003E\\n      \u003Cth\u003En't\u003C\u002Fth\u003E\\n      \u003Cth\u003ERB\u003C\u002Fth\u003E\\n      \u003Ctd\u003E1.250162\u003C\u002Ftd\u003E\\n    \u003C\u002Ftr\u003E\\n  \u003C\u002Ftbody\u003E\\n\u003C\u002Ftable\u003E\\n\u003C\u002Fdiv\u003E\\n\\n\\n\\n\u003Cp\u003ECompare the parts of the function given to \u003Ccode\u003Etransform()\u003C\u002Fcode\u003E with the equation:\u003C\u002Fp\u003E\\n\u003Cp\u003E$$ IDF_w = log(1 + \\\\frac{N}{df_w}) $$\u003C\u002Fp\u003E\\n\u003Cp\u003EN is the total number of pages. Document frequency, $$df_w$$, is &#39;how many pages (docs) does the word occur on?&#39; That is the \u003Ccode\u003Ex.count()\u003C\u002Fcode\u003E. Can you modify the above to use corpus frequency, which is &#39;how many times does the word occur overall in the corpus (i.e. across all pages)?&#39; You&#39;d want to add everything up.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"more-features-in-the-htrc-extracted-features-dataset\\\"\u003EMore Features in the HTRC Extracted Features Dataset\u003C\u002Fh1\u003E\\n\u003Cp\u003ESo far we have mainly used token-counting features, accessed through \u003Ccode\u003EVolume.tokenlist()\u003C\u002Fcode\u003E. The HTRC Extracted Features Dataset provides more features at the volume level. Here are other features that are available to Volume objects. Try them on \u003Ccode\u003Evol\u003C\u002Fcode\u003E and see what the output is:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003E\u003Ccode\u003Evol.line_counts()\u003C\u002Fcode\u003E: How many vertically spaced lines of text, a measure related to the phyical format of the page.\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003Evol.sentence_counts()\u003C\u002Fcode\u003E: How many sentences of text: a measure related to the content on a page.\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003Evol.empty_line_counts()\u003C\u002Fcode\u003E: How many larger vertical spaces are there on the page between lines of text? In many cases, this can be used as a proxy for paragraph count. This is based on what software was used to OCR so there are inconsistencies: not all scans in the HathiTrust are OCR&#39;d identically.\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003Evol.begin_line_chars()\u003C\u002Fcode\u003E, \u003Ccode\u003Evol.end_line_chars()\u003C\u002Fcode\u003E: The count of different characters along the left-most and right-most sides of a page. This can tell you about what kind of page it is: for example, a table of contents might have a lot of numbers or roman numerals at the end of each line\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003EEarlier, we saw that the number of words on a page gave some indication of whether it was a page of the story or a different kind of page (chapter, front matter, etc). We can see that line count is another contextual &#39;hint&#39;:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Eline_counts = vol.line_counts()\\nplt.plot(line_counts)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;draft_52_1.png&quot; caption=&quot;Output graph.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EThe majority of pages have 20-25 lines, confirmable with a histogram: \u003Ccode\u003Eplt.hist(line_counts)\u003C\u002Fcode\u003E. This is likely what a full page of text looks like in this book. A scholar trying to focus on patterns only in the text and comfortable missing a few short pages might choose to filter to just these pages.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"page-level-features\\\"\u003EPage-Level Features\u003C\u002Fh2\u003E\\n\u003Cp\u003EIf you open the raw dataset file for a HTRC EF volume on your computer, you may notice that features are provided for each page. While this lesson has focused on volumes, most of the features that we have seen can be accessed for a single page; e.g. \u003Ccode\u003EPage.tokenlist()\u003C\u002Fcode\u003E instead of \u003Ccode\u003EVolume.tokenlist()\u003C\u002Fcode\u003E. The methods to access the features are named the same, with the exception that \u003Ccode\u003Eline_count\u003C\u002Fcode\u003E, \u003Ccode\u003Eempty_line_count\u003C\u002Fcode\u003E, and \u003Ccode\u003Esentence_count\u003C\u002Fcode\u003E are not pluralized.\u003C\u002Fp\u003E\\n\u003Cp\u003ELike iterating over \u003Ccode\u003EFeatureReader.volumes()\u003C\u002Fcode\u003E to get Volume objects, it is possible to iterate across pages with \u003Ccode\u003EVolume.pages()\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"next-steps\\\"\u003ENext Steps\u003C\u002Fh1\u003E\\n\u003Cp\u003ENow that you know the basics of the HTRC Feature Reader, you can learn more about the \u003Ca href=\\\"https:\u002F\u002Fanalytics.hathitrust.org\u002Ffeatures\\\"\u003EExtracted Features dataset\u003C\u002Fa\u003E. The \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fhtrc\u002Fhtrc-feature-reader\u002Fblob\u002Fmaster\u002FREADME.ipynb\\\"\u003EFeature Reader home page\u003C\u002Fa\u003E contains a lesson similar to this one but for more advanced users (that&#39;s you now!), and the \u003Ca href=\\\"http:\u002F\u002Fhtrc.github.io\u002Fhtrc-feature-reader\u002Fhtrc_features\u002Ffeature_reader.m.html\\\"\u003Ecode documentation\u003C\u002Fa\u003E gives exact information about what types of information can be called.\u003C\u002Fp\u003E\\n\u003Cp\u003EUnderwood (2015) has released \u003Ca href=\\\"https:\u002F\u002Fanalytics.hathitrust.org\u002Fgenre\\\"\u003Egenre classifications of public-domain texts in the HTRC EF Dataset\u003C\u002Fa\u003E, comprised of fiction, poetry, and drama. Though many historians will be interested in other corners of the dataset, fiction is a good place to tinker with text mining ideas because of its expressiveness and relative format consistency.\u003C\u002Fp\u003E\\n\u003Cp\u003EFinally, the repository for the HTRC Feature Reader has \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fhtrc\u002Fhtrc-feature-reader\u002Ftree\u002Fmaster\u002Fexamples\\\"\u003Eadvanced tutorial notebooks\u003C\u002Fa\u003E showing how to use the library further. One such tutorial shows how to \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fhtrc\u002Fhtrc-feature-reader\u002Fblob\u002Fmaster\u002Fexamples\u002FWithin-Book%20Sentiment%20Trends.ipynb\\\"\u003Ederive &#39;plot arcs&#39; for a text\u003C\u002Fa\u003E, a process popularized by Jockers (2015).\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;plot-arc.png&quot; caption=&quot;Plot Arc Example.&quot; %}\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"references\\\"\u003EReferences\u003C\u002Fh1\u003E\\n\u003Cp\u003EBoris Capitanu, Ted Underwood, Peter Organisciak, Timothy Cole, Maria Janina Sarol, J. Stephen Downie (2016). The HathiTrust Research Center Extracted Feature Dataset (1.0) [Dataset]. HathiTrust Research Center, \u003Ca href=\\\"https:\u002F\u002Fdoi.org\u002F10.13012\u002FJ8X63JT3\\\"\u003Ehttps:\u002F\u002Fdoi.org\u002F10.13012\u002FJ8X63JT3\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EChris Forster. &quot;A Walk Through the Metadata: Gender in the HathiTrust Dataset.&quot; Blog. \u003Ca href=\\\"http:\u002F\u002Fcforster.com\u002F2015\u002F09\u002Fgender-in-hathitrust-dataset\u002F\\\"\u003Ehttp:\u002F\u002Fcforster.com\u002F2015\u002F09\u002Fgender-in-hathitrust-dataset\u002F\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EMatthew L. Jockers (Feb 2015). &quot;Revealing Sentiment and Plot Arcs with the Syuzhet Package&quot;. \u003Cem\u003EMatthew L. Jockers\u003C\u002Fem\u003E. Blog. \u003Ca href=\\\"http:\u002F\u002Fwww.matthewjockers.net\u002F2015\u002F02\u002F02\u002Fsyuzhet\u002F\\\"\u003Ehttp:\u002F\u002Fwww.matthewjockers.net\u002F2015\u002F02\u002F02\u002Fsyuzhet\u002F\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EPeter Organisciak, Loretta Auvil, J. Stephen Downie (2015). â€œRemembering books: A within-book topic mapping technique.â€ Digital Humanities 2015. Sydney, Australia.\u003C\u002Fp\u003E\\n\u003Cp\u003EStÃ©fan Sinclair &amp; Geoffrey Rockwell (2016). &quot;The Art of Literary Text Analysis.&quot; Github.com. Commit b04bc18. \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fsgsinclair\u002Falta\\\"\u003Ehttps:\u002F\u002Fgithub.com\u002Fsgsinclair\u002Falta\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EWilliam J. Turkel and Adam Crymble (2012). &quot;Counting Word Frequencies with Python&quot;. The Programming Historian. \u002Flessons\u002Fcounting-frequencies.\u003C\u002Fp\u003E\\n\u003Cp\u003ETed Underwood (2014): Understanding Genre in a Collection of a Million Volumes, Interim Report. figshare.\\n\u003Ca href=\\\"https:\u002F\u002Fdoi.org\u002F10.6084\u002Fm9.figshare.1281251.v1\\\"\u003Ehttps:\u002F\u002Fdoi.org\u002F10.6084\u002Fm9.figshare.1281251.v1\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003ETed Underwood, Boris Capitanu, Peter Organisciak, Sayan Bhattacharyya, Loretta Auvil, Colleen Fallaw, J. Stephen Downie (2015). &quot;Word Frequencies in English-Language Literature, 1700-1922&quot; (0.2) [Dataset]. \u003Cem\u003EHathiTrust Research Center\u003C\u002Fem\u003E. \u003Ca href=\\\"https:\u002F\u002Fdoi.org\u002F10.13012\u002FJ8JW8BSJ\\\"\u003Ehttps:\u002F\u002Fdoi.org\u002F10.13012\u002FJ8JW8BSJ\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EHadley Wickham (2011). &quot;The split-apply-combine strategy for data analysis&quot;. \u003Cem\u003EJournal of Statistical Software\u003C\u002Fem\u003E, 40(1), 1-29.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"appendix-downloading-custom-files-via-rsync\\\"\u003EAppendix: Downloading custom files via rsync\u003C\u002Fh1\u003E\\n\u003Cp\u003EThe full HTRC Extracted Features dataset is accessible using \u003Cem\u003Ersync\u003C\u002Fem\u003E, a Unix command line program for syncing files. It is already preinstalled on Linux or Mac OS. Windows users need to use \u003Cem\u003Ersync\u003C\u002Fem\u003E by downloading a program such as \u003Ca href=\\\"https:\u002F\u002Fcygwin.com\u002F\\\"\u003ECygwin\u003C\u002Fa\u003E, which provides a Unix-like command line environment in Windows.\u003C\u002Fp\u003E\\n\u003Cp\u003ETo download all \u003Cem\u003E4 TB\u003C\u002Fem\u003E comprising the EF dataset, you can use this command (be aware the full transfer will take a very long time):\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Ersync -rv data.analytics.hathitrust.org::features\u002F .\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThis command recurses (the \u003Ccode\u003E-r\u003C\u002Fcode\u003E flag) through all the folders on the HTRC server, and syncs all the files to a location on your system; in this case the \u003Ccode\u003E.\u003C\u002Fcode\u003E at the end means &quot;the current folder&quot;. The \u003Ccode\u003E-v\u003C\u002Fcode\u003E flag means \u003Ccode\u003E--verbose\u003C\u002Fcode\u003E, which tells rsync to show you more information.\u003C\u002Fp\u003E\\n\u003Cp\u003EIt is possible to sync individual files by specifying a full file path. Files are organized in a \u003Ca href=\\\"https:\u002F\u002Fwiki.ucop.edu\u002Fdisplay\u002FCuration\u002FPairTree\\\"\u003EPairTree structure\u003C\u002Fa\u003E, meaning that you can find an exact dataset file from a volume&#39;s HathiTrust id. The HTRC Feature Reader has a tools and instructions for \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fhtrc\u002Fhtrc-feature-reader\u002Fblob\u002Fmaster\u002Fexamples\u002FID_to_Rsync_Link.ipynb\\\"\u003Egetting the path for a volume\u003C\u002Fa\u003E. A list of all file paths is available:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Ersync -azv data.analytics.hathitrust.org::features\u002Flisting\u002Fhtrc-ef-all-files.txt .\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EFinally, it is possible to download many files from a list. To try, we&#39;ve put together lists for public-domain \u003Ca href=\\\"http:\u002F\u002Fdata.analytics.hathitrust.org\u002Fgenre\u002Ffiction_paths.txt\\\"\u003Efiction\u003C\u002Fa\u003E, \u003Ca href=\\\"http:\u002F\u002Fdata.analytics.hathitrust.org\u002Fgenre\u002Fdrama_paths.txt\\\"\u003Edrama\u003C\u002Fa\u003E, and \u003Ca href=\\\"http:\u002F\u002Fdata.analytics.hathitrust.org\u002Fgenre\u002Fpoetry_paths.txt\\\"\u003Epoetry\u003C\u002Fa\u003E (Underwood 2014). For example:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Ersync -azv --files-from=fiction_paths.txt data.analytics.hathitrust.org::features\u002F .\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\"}"}</script></div>
	</body>
</html>
